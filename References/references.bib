@article{zhang2017deeper,
  author  = "Zhang, Shangtong and Sutton, Richard S",
  journal = "arXiv preprint arXiv:1712.01275",
  title   = "A deeper look at experience replay",
  year    = "2017",
}



@inproceedings{zhao2020sim,
  author       = "Zhao, Wenshuai and Queralta, Jorge Pe{\~n}a and Westerlund, Tomi",
  booktitle    = "2020 IEEE symposium series on computational intelligence (SSCI)",
  organization = "IEEE",
  pages        = "737--744",
  title        = "Sim-to-real transfer in deep reinforcement learning for robotics: a survey",
  year         = "2020",
}
@article{liu2022digital,
  author    = "Liu, Yongkui and Xu, He and Liu, Ding and Wang, Lihui",
  journal   = "Robotics and Computer-Integrated Manufacturing",
  pages     = "102365",
  publisher = "Elsevier",
  title     = "A digital twin-based sim-to-real transfer for deep reinforcement learning-enabled industrial robot grasping",
  volume    = "78",
  year      = "2022",
}
@inproceedings{qin2023dexpoint,
  author       = "Qin, Yuzhe and Huang, Binghao and Yin, Zhao-Heng and Su, Hao and Wang, Xiaolong",
  booktitle    = "Conference on Robot Learning",
  organization = "PMLR",
  pages        = "594--605",
  title        = "Dexpoint: Generalizable point cloud reinforcement learning for sim-to-real dexterous manipulation",
  year         = "2023",
}



@article{Mine_DAIL,
  author    = "Nguyen Duc, Tho and Tran, Chanh Minh and Tan, Phan Xuan and Kamioka, Eiji",
  copyright = "http://creativecommons.org/licenses/by/3.0/",
  doi       = "10.3390/s21144718",
  groups    = "Mine",
  issn      = "1424-8220",
  journal   = "Sensors",
  keywords  = "imitation learning, domain adaptive imitation learning, generative adversarial network",
  language  = "en",
  month     = jan,
  number    = "14",
  pages     = "4718",
  publisher = "Multidisciplinary Digital Publishing Institute",
  title     = "Domain {Adaptation} for {Imitation} {Learning} {Using} {Generative} {Adversarial} {Network}",
  url       = "https://www.mdpi.com/1424-8220/21/14/4718",
  volume    = "21",
  year      = "2021",
}


@article{Mine_DTAIL,
  author       = "Nguyen Duc, Tho and Tran, Chanh Minh and Bach, Nguyen Gia and Tan, Phan Xuan and Kamioka, Eiji",
  copyright    = "http://creativecommons.org/licenses/by/3.0/",
  date         = "2022-01",
  doi          = "10.3390/s22186959",
  groups       = "Mine",
  issn         = "1424-8220",
  journaltitle = "Sensors",
  keywords     = "imitation learning, task adaptation, repetition learning, transfer learning, generative adversarial network",
  language     = "en",
  number       = "18",
  pages        = "6959",
  publisher    = "Multidisciplinary Digital Publishing Institute",
  title        = "Repetition-{Based} {Approach} for {Task} {Adaptation} in {Imitation} {Learning}",
  url          = "https://www.mdpi.com/1424-8220/22/18/6959",
  volume       = "22",
}


@article{Neuroscience_Ebbinghaus2013,
  author       = "Ebbinghaus, Hermann",
  date         = "2013-10",
  doi          = "10.5214/ans.0972.7531.200408",
  file         = ":Neuroscience_Ebbinghaus2013 - Memory_ a Contribution to Experimental Psychology.html:URL",
  groups       = "Neuroscience",
  issn         = "0972-7531",
  journaltitle = "Annals of Neurosciences",
  language     = "eng",
  number       = "4",
  pages        = "155--156",
  pmcid        = "PMC4117135",
  pmid         = "25206041",
  shorttitle   = "Memory",
  title        = "Memory: a contribution to experimental psychology",
  volume       = "20",
}


@article{Neuroscience_Uchihara2019,
  abstract     = "This meta-analysis aimed to clarify the complex relationship between repetition and second language (L2) incidental vocabulary learning by meta-analyzing primary studies reporting correlation coefficients between the number of encounters and vocabulary learning. We synthesized and quantitatively analyzed 45 effect sizes from 26 studies (N = 1,918) to calculate the mean effect size of the frequency–learning relationship and to explore the extent to which 10 empirically motivated variables moderate this relationship. Results showed that there was a medium effect (r = .34) of repetition on incidental vocabulary learning. Subsequent moderator analyses revealed that variability in the size of repetition effects across studies was explained by learner variables (age, vocabulary knowledge), treatment variables (spaced learning, visual support, engagement, range in number of encounters), and methodological differences (nonword use, forewarning of an upcoming comprehension test, vocabulary test format). Based on the findings, we suggest future directions for L2 incidental vocabulary learning research. Open Practices This article has been awarded an Open Data badge. All data are publicly accessible via the Open Science Framework at https://osf.io/rmnk2. Learn more about the Open Practices badges from the Center for Open Science: https://osf.io/tvyxz/wiki.",
  author       = "Uchihara, Takumi and Webb, Stuart and Yanagisawa, Akifumi",
  date         = "2019",
  doi          = "10.1111/lang.12343",
  file         = "Full Text PDF:https\://onlinelibrary.wiley.com/doi/pdfdirect/10.1111/lang.12343:application/pdf",
  groups       = "Neuroscience",
  issn         = "1467-9922",
  journaltitle = "Language Learning",
  keywords     = "frequency, vocabulary learning, incidental, second language, meta-analysis",
  language     = "en",
  number       = "3",
  pages        = "559--599",
  shorttitle   = "The {Effects} of {Repetition} on {Incidental} {Vocabulary} {Learning}",
  title        = "The {Effects} of {Repetition} on {Incidental} {Vocabulary} {Learning}: {A} {Meta}-{Analysis} of {Correlational} {Studies}",
  url          = "https://onlinelibrary.wiley.com/doi/abs/10.1111/lang.12343",
  urldate      = "2023-02-16",
  volume       = "69",
}


@article{Neuroscience_Wozniak1995,
  abstract     = "The existence of two independent components of long-term memory has been demonstrated by the authors. The evidence has been derived from the authors' findings related to the optimum spacing of repetitions in paired-associate learning. The two components are sufficient to explain the optimum spacing of repetitions as well as the spacing effect. Although the molecular counterparts of the two components of memory are not known, the authors provide a collection of guidelines that might facilitate identification of such counterparts.",
  author       = "Woźniak, P. A. and Gorzelańczyk, E. J. and Murakowski, J. A.",
  date         = "1995",
  file         = ":Neuroscience_Wozniak1995 - Two Components of Long Term Memory.html:URL",
  groups       = "Neuroscience",
  issn         = "0065-1400",
  journaltitle = "Acta Neurobiologiae Experimentalis",
  keywords     = "Humans, Memory",
  language     = "eng",
  number       = "4",
  pages        = "301--305",
  pmid         = "8713361",
  title        = "Two components of long-term memory",
  volume       = "55",
}


@article{Neuroscience_Zhan2018,
  abstract     = "When stimuli are learned by repetition, they are remembered better and retained for a longer time. However, current findings are lacking as to whether the medial temporal lobe (MTL) and cortical regions are involved in the learning effect when subjects retrieve associative memory, and whether their activations differentially change over time due to learning experience. To address these issues, we designed an fMRI experiment in which face-scene pairs were learned once (L1) or six times (L6). Subjects learned the pairs at four retention intervals, 30-min, 1-day, 1-week and 1-month, after which they finished an associative recognition task in the scanner. The results showed that compared to learning once, learning six times led to stronger activation in the hippocampus, but weaker activation in the perirhinal cortex (PRC) as well as anterior ventrolateral prefrontal cortex (vLPFC). In addition, the hippocampal activation was positively correlated with that of the parahippocampal place area (PPA) and negatively correlated with that of the vLPFC when the L6 group was compared to the L1 group. The hippocampal activation decreased over time after L1 but remained stable after L6. These results clarified how the hippocampus and cortical regions interacted to support associative memory after different learning experiences.",
  author       = "Zhan, Lexia and Guo, Dingrong and Chen, Gang and Yang, Jiongjiong",
  date         = "2018",
  doi          = "10.3389/fnhum.2018.00277",
  file         = ":Neuroscience_Zhan2018 - Effects of Repetition Learning on Associative Recognition Over Time_ Role of the Hippocampus and Prefrontal Cortex.html:URL",
  groups       = "Neuroscience",
  issn         = "1662-5161",
  journaltitle = "Frontiers in Human Neuroscience",
  keywords     = "MTL, PFC, associative memory, consolidation, repetition learning",
  language     = "eng",
  pages        = "277",
  pmcid        = "PMC6050388",
  pmid         = "30050418",
  shorttitle   = "Effects of {Repetition} {Learning} on {Associative} {Recognition} {Over} {Time}",
  title        = "Effects of {Repetition} {Learning} on {Associative} {Recognition} {Over} {Time}: {Role} of the {Hippocampus} and {Prefrontal} {Cortex}",
  volume       = "12",
}

@inproceedings{abbeel2004apprenticeship,
  author    = "Abbeel, Pieter and Ng, Andrew Y",
  booktitle = "Proceedings of the twenty-first international conference on Machine learning",
  date      = "2004",
  groups    = "Others",
  pages     = "1",
  title     = "Apprenticeship learning via inverse reinforcement learning",
}

@article{arora2021survey,
  author       = "Arora, Saurabh and Doshi, Prashant",
  date         = "2021",
  groups       = "Others",
  journaltitle = "Artificial Intelligence",
  pages        = "103500",
  publisher    = "Elsevier",
  title        = "A survey of inverse reinforcement learning: Challenges, methods and progress",
}


%=======================================================
% IL General
%=======================================================

@inproceedings{baker2007goal,
  author    = "Baker, Chris L and Tenenbaum, Joshua B and Saxe, Rebecca R",
  booktitle = "Proceedings of the Annual Meeting of the Cognitive Science Society",
  date      = "2007",
  groups    = "Others",
  number    = "29",
  title     = "Goal inference as inverse planning",
  volume    = "29",
}

@article{bao2017imitation,
  author       = "Bao, Yunqing and Cuijpers, Raymond H",
  date         = "2017",
  groups       = "Others",
  journaltitle = "International Journal of Social Robotics",
  number       = "5",
  pages        = "691--703",
  publisher    = "Springer",
  title        = "On the imitation of goal directed movements of a humanoid robot",
  volume       = "9",
}

@inproceedings{Baseline_NFQI,
  author       = "Riedmiller, Martin",
  booktitle    = "European conference on machine learning",
  date         = "2005",
  groups       = "Others",
  organization = "Springer",
  pages        = "317--328",
  title        = "Neural fitted Q iteration--first experiences with a data efficient neural reinforcement learning method",
}



%=======================================================
% RL Models
%=======================================================

@article{Baseline_PPO,
  author       = "Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg",
  date         = "2017",
  groups       = "Others",
  journaltitle = "arXiv preprint arXiv:1707.06347",
  title        = "Proximal policy optimization algorithms",
}


@inproceedings{behbahani2019learning,
  author       = "Behbahani, Feryal and Shiarlis, Kyriacos and Chen, Xi and Kurin, Vitaly and Kasewa, Sudhanshu and Stirbu, Ciprian and Gomes, Joao and Paul, Supratik and Oliehoek, Frans A and Messias, Joao and others",
  booktitle    = "2019 International Conference on Robotics and Automation (ICRA)",
  date         = "2019",
  groups       = "Others",
  organization = "IEEE",
  pages        = "775--781",
  title        = "Learning from demonstration in the wild",
}


@article{bing2020energy,
  author       = "Bing, Zhenshan and Lemke, Christian and Cheng, Long and Huang, Kai and Knoll, Alois",
  date         = "2020",
  groups       = "Others",
  journaltitle = "Neural Networks",
  pages        = "323--333",
  publisher    = "Elsevier",
  title        = "Energy-efficient and damage-recovery slithering gait design for a snake-like robot based on reinforcement learning and inverse reinforcement learning",
  volume       = "129",
}




@inproceedings{chi2020collaborative,
  author       = "Chi, Wenqiang and Dagnino, Giulio and Kwok, Trevor MY and Nguyen, Anh and Kundrat, Dennis and Abdelaziz, Mohamed EMK and Riga, Celia and Bicknell, Colin and Yang, Guang-Zhong",
  booktitle    = "2020 IEEE International Conference on Robotics and Automation (ICRA)",
  date         = "2020",
  groups       = "Others",
  organization = "IEEE",
  pages        = "2414--2420",
  title        = "Collaborative robot-assisted endovascular catheterization with generative adversarial imitation learning",
}



%=======================================================
% Contrastive learning
%=======================================================

@inproceedings{CL_Stopgrad_1,
  author       = "Tian, Yuandong and Chen, Xinlei and Ganguli, Surya",
  booktitle    = "International Conference on Machine Learning",
  date         = "2021",
  groups       = "Others",
  organization = "PMLR",
  pages        = "10268--10278",
  title        = "Understanding self-supervised learning dynamics without contrastive pairs",
}

@inproceedings{CL_Stopgrad_2,
  author    = "Chen, Xinlei and He, Kaiming",
  booktitle = "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
  date      = "2021",
  groups    = "Others",
  pages     = "15750--15758",
  title     = "Exploring simple siamese representation learning",
}

%=======================================================
% IL DA Models
%=======================================================

@inproceedings{DAIL_Model_DAIL,
  author    = "Kim, Kuno and Gu, Yihong and Song, Jiaming and Zhao, Shengjia and Ermon, Stefano",
  booktitle = "Proceedings of the 37th International Conference on Machine Learning",
  date      = "2020-11",
  groups    = "Others",
  issn      = "2640-3498",
  pages     = "5286--5295",
  publisher = "PMLR",
  title     = "{Domain Adaptive Imitation Learning}",
  url       = "http://proceedings.mlr.press/v119/kim20c.html",
  volume    = "119",
}

@inproceedings{DAIL_Model_ThirdPerson,
  arxivid    = "1703.01703",
  author     = "Stadie, Bradly C. and Abbeel, Pieter and Sutskever, Ilya",
  booktitle  = "International Conference on Learning Representations (ICLR)",
  date       = "2017-04",
  eprint     = "1703.01703",
  eprinttype = "arXiv",
  groups     = "Others",
  location   = "Toulon, France",
  title      = "{Third-Person Imitation Learning}",
  url        = "http://arxiv.org/abs/1703.01703",
}



%=======================================================
% DL Frameworks
%=======================================================

@article{DL_Lib_PyTorch,
  author       = "Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others",
  date         = "2019",
  groups       = "Others",
  journaltitle = "Advances in neural information processing systems",
  title        = "Pytorch: An imperative style, high-performance deep learning library",
  volume       = "32",
}

@article{DL_Lib_StableBaselines3,
  author       = "Raffin, Antonin and Hill, Ashley and Gleave, Adam and Kanervisto, Anssi and Ernestus, Maximilian and Dormann, Noah",
  date         = "2021",
  groups       = "Others",
  journaltitle = "Journal of Machine Learning Research",
  publisher    = "MIT Press",
  title        = "Stable-baselines3: Reliable reinforcement learning implementations",
}

@article{DL_Lib_Tianshou,
  author       = "Weng, Jiayi and Chen, Huayu and Yan, Dong and You, Kaichao and Duburcq, Alexis and Zhang, Minghao and Su, Hang and Zhu, Jun",
  date         = "2021",
  groups       = "Others",
  journaltitle = "arXiv preprint arXiv:2107.14171",
  title        = "Tianshou: A Highly Modularized Deep Reinforcement Learning Library",
}

@article{duan2017one,
  author       = "Duan, Yan and Andrychowicz, Marcin and Stadie, Bradly C and Ho, Jonathan and Schneider, Jonas and Sutskever, Ilya and Abbeel, Pieter and Zaremba, Wojciech",
  date         = "2017",
  groups       = "Others",
  journaltitle = "arXiv preprint arXiv:1703.07326",
  title        = "One-shot imitation learning",
}

@inproceedings{Env_MetaWorld,
  author    = "Yu, Tianhe and Quillen, Deirdre and He, Zhanpeng and Julian, Ryan and Hausman, Karol and Finn, Chelsea and Levine, Sergey",
  booktitle = "Proceedings of the Conference on Robot Learning",
  date      = "2020",
  editor    = "Kaelbling, Leslie Pack and Kragic, Danica and Sugiura, Komei",
  groups    = "Others",
  pages     = "1094--1100",
  pdf       = "http://proceedings.mlr.press/v100/yu20a/yu20a.pdf",
  publisher = "PMLR",
  series    = "Proceedings of Machine Learning Research",
  title     = "Meta-World: A Benchmark and Evaluation for Multi-Task and Meta Reinforcement Learning",
  url       = "https://proceedings.mlr.press/v100/yu20a.html",
  volume    = "100",
}

@inproceedings{finn2016guided,
  author       = "Finn, Chelsea and Levine, Sergey and Abbeel, Pieter",
  booktitle    = "International conference on machine learning",
  date         = "2016",
  groups       = "Others",
  organization = "PMLR",
  pages        = "49--58",
  title        = "Guided cost learning: Deep inverse optimal control via policy optimization",
}

@article{GAN_CAPTCHA,
  author       = "Hyun KWON and Yongchul KIM and Hyunsoo YOON and Daeseon CHOI",
  date         = "2018",
  doi          = "10.1587/transinf.2017EDL8175",
  groups       = "Others",
  journaltitle = "IEICE Transactions on Information and Systems",
  number       = "2",
  pages        = "543--546",
  title        = "CAPTCHA Image Generation Systems Using Generative Adversarial Networks",
  volume       = "E101.D",
}

@article{GAN_ImageCompression,
  author       = "Song, Jingkuan and He, Tao and Gao, Lianli and Xu, Xing and Hanjalic, Alan and Shen, Heng Tao",
  date         = "2020",
  groups       = "Others",
  journaltitle = "International Journal of Computer Vision",
  pages        = "1--22",
  publisher    = "Springer",
  title        = "Unified binary generative adversarial network for image retrieval and compression",
}

@inproceedings{GAN_ImageTranslation,
  author    = "Chen, Hanting and Wang, Yunhe and Shu, Han and Wen, Changyuan and Xu, Chunjing and Shi, Boxin and Xu, Chao and Xu, Chang",
  booktitle = "Proceedings of the AAAI Conference on Artificial Intelligence",
  date      = "2020",
  groups    = "Others",
  number    = "04",
  pages     = "3585--3592",
  title     = "Distilling portable generative adversarial networks for image translation",
  volume    = "34",
}

@article{GAN_Original,
  archiveprefix = "arXiv",
  arxivid       = "1406.2661",
  author        = "Goodfellow, Ian J and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua",
  eprint        = "1406.2661",
  journal       = "arXiv",
  month         = "jun",
  title         = "{Generative Adversarial Networks}",
  url           = "http://arxiv.org/abs/1406.2661",
  year          = "2014",
}

@article{GAN_MRI,
  author       = "Qi, Mengke and Li, Yongbao and Wu, Aiqian and Jia, Qiyuan and Li, Bin and Sun, Wenzhao and Dai, Zhenhui and Lu, Xingyu and Zhou, Linghong and Deng, Xiaowu and others",
  date         = "2020",
  groups       = "Others",
  journaltitle = "Medical physics",
  number       = "4",
  pages        = "1880--1894",
  publisher    = "Wiley Online Library",
  title        = "Multi-sequence MR image-based synthetic CT generation using a generative adversarial network for head and neck MRI-only radiotherapy",
  volume       = "47",
}


% autonomous driving
@inproceedings{IL_Driving_1,
  author       = "Chen, Jianyu and Yuan, Bodi and Tomizuka, Masayoshi",
  booktitle    = "2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)",
  date         = "2019",
  groups       = "Others",
  organization = "IEEE",
  pages        = "2884--2890",
  title        = "Deep imitation learning for autonomous driving in generic urban scenarios with enhanced safety",
}

@inproceedings{IL_Driving_2,
  author       = "Codevilla, Felipe and M{\"u}ller, Matthias and L{\'o}pez, Antonio and Koltun, Vladlen and Dosovitskiy, Alexey",
  booktitle    = "2018 IEEE international conference on robotics and automation (ICRA)",
  date         = "2018",
  groups       = "Others",
  organization = "IEEE",
  pages        = "4693--4700",
  title        = "End-to-end driving via conditional imitation learning",
}

@inproceedings{IL_Driving_3,
  author       = "Hawke, Jeffrey and Shen, Richard and Gurau, Corina and Sharma, Siddharth and Reda, Daniele and Nikolov, Nikolay and Mazur, Przemys{\l}aw and Micklethwaite, Sean and Griffiths, Nicolas and Shah, Amar and others",
  booktitle    = "2020 IEEE International Conference on Robotics and Automation (ICRA)",
  date         = "2020",
  groups       = "Others",
  organization = "IEEE",
  pages        = "251--257",
  title        = "Urban driving with conditional imitation learning",
}

@inproceedings{IL_Driving_4,
  author       = "Kebria, Parham M and Alizadehsani, Roohallah and Salaken, Syed Moshfeq and Hossain, Ibrahim and Khosravi, Abbas and Kabir, Dipu and Koohestani, Afsaneh and Asadi, Houshyar and Nahavandi, Saeid and Tunsel, Edward and others",
  booktitle    = "2019 IEEE International Conference on Industrial Technology (ICIT)",
  date         = "2019",
  groups       = "Others",
  organization = "IEEE",
  pages        = "865--870",
  title        = "Evaluating architecture impacts on deep imitation learning performance for autonomous driving",
}



%=======================================================
% IL Applications
%=======================================================

% robotics control
@inproceedings{IL_Robotics_1,
  author       = "Jang, Eric and Irpan, Alex and Khansari, Mohi and Kappler, Daniel and Ebert, Frederik and Lynch, Corey and Levine, Sergey and Finn, Chelsea",
  booktitle    = "Conference on Robot Learning",
  date         = "2022",
  groups       = "Others",
  organization = "PMLR",
  pages        = "991--1002",
  title        = "BC-z: Zero-shot task generalization with robotic imitation learning",
}

@article{IL_Robotics_2,
  author       = "Zhu, Yuke and Wang, Ziyu and Merel, Josh and Rusu, Andrei and Erez, Tom and Cabi, Serkan and Tunyasuvunakool, Saran and Kram{\'a}r, J{\'a}nos and Hadsell, Raia and de Freitas, Nando and others",
  date         = "2018",
  groups       = "Others",
  journaltitle = "arXiv preprint arXiv:1802.09564",
  title        = "Reinforcement and imitation learning for diverse visuomotor skills",
}

@inproceedings{IL_Robotics_3,
  author       = "Ratliff, Nathan and Bagnell, J Andrew and Srinivasa, Siddhartha S",
  booktitle    = "2007 7th IEEE-RAS International Conference on Humanoid Robots",
  date         = "2007",
  groups       = "Others",
  organization = "IEEE",
  pages        = "392--397",
  title        = "Imitation learning for locomotion and manipulation",
}

@article{IL_Survey_2,
  author       = "Hussein, Ahmed and Gaber, Mohamed Medhat and Elyan, Eyad and Jayne, Chrisina",
  date         = "2017",
  groups       = "Others",
  journaltitle = "ACM Computing Surveys (CSUR)",
  number       = "2",
  pages        = "1--35",
  publisher    = "ACM New York, NY, USA",
  title        = "Imitation learning: A survey of learning methods",
  volume       = "50",
}


@inproceedings{kalakrishnan2013learning,
  author       = "Kalakrishnan, Mrinal and Pastor, Peter and Righetti, Ludovic and Schaal, Stefan",
  booktitle    = "2013 IEEE International Conference on Robotics and Automation",
  date         = "2013",
  groups       = "Others",
  organization = "IEEE",
  pages        = "1331--1336",
  title        = "Learning objective functions for manipulation",
}

@article{levine2011nonlinear,
  author       = "Levine, Sergey and Popovic, Zoran and Koltun, Vladlen",
  date         = "2011",
  groups       = "Others",
  journaltitle = "Advances in neural information processing systems",
  pages        = "19--27",
  title        = "Nonlinear inverse reinforcement learning with gaussian processes",
  volume       = "24",
}


@inproceedings{Liu2018,
  arxivid         = "1707.03374",
  author          = "Liu, Yuxuan and Gupta, Abhishek and Abbeel, Pieter and Levine, Sergey",
  booktitle       = "IEEE International Conference on Robotics and Automation (ICRA)",
  date            = "2018-09",
  doi             = "10.1109/ICRA.2018.8462901",
  eprint          = "1707.03374",
  eprinttype      = "arXiv",
  groups          = "Others",
  isbn            = "9781538630815",
  issn            = "10504729",
  mendeley-groups = "_Imitation Learning/_UNREAD",
  pages           = "1118--1125",
  publisher       = "Institute of Electrical and Electronics Engineers Inc.",
  title           = "{Imitation from Observation: Learning to Imitate Behaviors from Raw Video via Context Translation}",
}


@article{Liu2019,
  arxivid      = "1911.10947",
  author       = "Liu, Fangchen and Ling, Zhan and Mu, Tongzhou and Su, Hao",
  date         = "2019-11",
  eprint       = "1911.10947",
  eprinttype   = "arXiv",
  groups       = "Others",
  journaltitle = "arXiv",
  publisher    = "arXiv",
  title        = "{State Alignment-based Imitation Learning}",
  url          = "http://arxiv.org/abs/1911.10947",
}

@article{Memory_Effect_1,
  author       = "Ebbinghaus, Hermann",
  date         = "2013",
  groups       = "Others",
  journaltitle = "Annals of neurosciences",
  number       = "4",
  pages        = "155",
  publisher    = "SAGE Publications",
  title        = "Memory: A contribution to experimental psychology",
  volume       = "20",
}

@article{Memory_Repetition_2,
  author       = "Uchihara, Takumi and Webb, Stuart and Yanagisawa, Akifumi",
  date         = "2019",
  groups       = "Others",
  journaltitle = "Language Learning",
  number       = "3",
  pages        = "559--599",
  publisher    = "Wiley Online Library",
  title        = "The effects of repetition on incidental vocabulary learning: A meta-analysis of correlational studies",
  volume       = "69",
}

@inproceedings{MTL_1,
  author       = "Guo, Zhaohan Daniel and Pires, Bernardo Avila and Piot, Bilal and Grill, Jean-Bastien and Altch{\'e}, Florent and Munos, R{\'e}mi and Azar, Mohammad Gheshlaghi",
  booktitle    = "International Conference on Machine Learning",
  date         = "2020",
  groups       = "Others",
  organization = "PMLR",
  pages        = "3875--3886",
  title        = "Bootstrap latent-predictive representations for multitask reinforcement learning",
}

@article{MTL_3,
  author       = "Teh, Yee and Bapst, Victor and Czarnecki, Wojciech M and Quan, John and Kirkpatrick, James and Hadsell, Raia and Heess, Nicolas and Pascanu, Razvan",
  date         = "2017",
  groups       = "Others",
  journaltitle = "Advances in neural information processing systems",
  title        = "Distral: Robust multitask reinforcement learning",
  volume       = "30",
}

@inproceedings{MTL_4,
  author       = "Espeholt, Lasse and Soyer, Hubert and Munos, Remi and Simonyan, Karen and Mnih, Vlad and Ward, Tom and Doron, Yotam and Firoiu, Vlad and Harley, Tim and Dunning, Iain and others",
  booktitle    = "International Conference on Machine Learning",
  date         = "2018",
  groups       = "Others",
  organization = "PMLR",
  pages        = "1407--1416",
  title        = "Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures",
}

@inproceedings{MTL_5,
  author    = "Hessel, Matteo and Soyer, Hubert and Espeholt, Lasse and Czarnecki, Wojciech and Schmitt, Simon and van Hasselt, Hado",
  booktitle = "Proceedings of the AAAI Conference on Artificial Intelligence",
  date      = "2019",
  groups    = "Others",
  number    = "01",
  pages     = "3796--3803",
  title     = "Multi-task deep reinforcement learning with popart",
  volume    = "33",
}

@inproceedings{Mujoco_HAPTIX,
  author       = "Kumar, Vikash and Todorov, Emanuel",
  booktitle    = "2015 IEEE-RAS 15th International Conference on Humanoid Robots (Humanoids)",
  date         = "2015",
  groups       = "Others",
  organization = "IEEE",
  pages        = "657--663",
  title        = "Mujoco haptix: A virtual reality system for hand manipulation",
}

@inproceedings{naumann2020analyzing,
  author       = "Naumann, Maximilian and Sun, Liting and Zhan, Wei and Tomizuka, Masayoshi",
  booktitle    = "2020 IEEE International Conference on Robotics and Automation (ICRA)",
  date         = "2020",
  groups       = "Others",
  organization = "IEEE",
  pages        = "5481--5487",
  title        = "Analyzing the Suitability of Cost Functions for Explaining and Imitating Human Driving Behavior based on Inverse Reinforcement Learning",
}

@inproceedings{ng2000algorithms,
  author    = "Ng, Andrew Y and Russell, Stuart J and others",
  booktitle = "ICML",
  date      = "2000",
  groups    = "Others",
  pages     = "2",
  title     = "Algorithms for inverse reinforcement learning.",
  volume    = "1",
}


@inproceedings{pmlr-v70-baram17a,
  abstract  = "Generative Adversarial Networks (GANs) have been successfully applied to the problem of policy imitation in a model-free setup. However, the computation graph of GANs, that include a stochastic policy as the generative model, is no longer differentiable end-to-end, which requires the use of high-variance gradient estimation. In this paper, we introduce the Model-based Generative Adversarial Imitation Learning (MGAIL) algorithm. We show how to use a forward model to make the computation fully differentiable, which enables training policies using the exact gradient of the discriminator. The resulting algorithm trains competent policies using relatively fewer expert samples and interactions with the environment. We test it on both discrete and continuous action domains and report results that surpass the state-of-the-art.",
  author    = "Nir Baram and Oron Anschel and Itai Caspi and Shie Mannor",
  booktitle = "Proceedings of the 34th International Conference on Machine Learning",
  date      = "2017",
  editor    = "Precup, Doina and Teh, Yee Whye",
  groups    = "Others",
  pages     = "390--399",
  pdf       = "http://proceedings.mlr.press/v70/baram17a/baram17a.pdf",
  publisher = "PMLR",
  series    = "Proceedings of Machine Learning Research",
  title     = "End-to-End Differentiable Adversarial Imitation Learning",
  url       = "http://proceedings.mlr.press/v70/baram17a.html",
  volume    = "70",
}



@techreport{pomerleau1989alvinn,
  author      = "Pomerleau, Dean A",
  date        = "1989",
  groups      = "Others",
  institution = "CARNEGIE-MELLON UNIV PITTSBURGH PA ARTIFICIAL INTELLIGENCE AND PSYCHOLOGY~…",
  title       = "Alvinn: An autonomous land vehicle in a neural network",
}

@article{RL_Challenges_1,
  author       = "Dulac-Arnold, Gabriel and Levine, Nir and Mankowitz, Daniel J and Li, Jerry and Paduraru, Cosmin and Gowal, Sven and Hester, Todd",
  date         = "2021",
  groups       = "Others",
  journaltitle = "Machine Learning",
  number       = "9",
  pages        = "2419--2468",
  publisher    = "Springer",
  title        = "Challenges of real-world reinforcement learning: definitions, benchmarks and analysis",
  volume       = "110",
}

@article{RL_Challenges_2,
  author       = "Kormushev, Petar and Calinon, Sylvain and Caldwell, Darwin G",
  date         = "2013",
  groups       = "Others",
  journaltitle = "Robotics",
  number       = "3",
  pages        = "122--148",
  publisher    = "Multidisciplinary Digital Publishing Institute",
  title        = "Reinforcement learning in robotics: Applications and real-world challenges",
  volume       = "2",
}


%=======================================================
% RL Models
%=======================================================

@article{RL_DDPG,
  author       = "Lillicrap, Timothy P and Hunt, Jonathan J and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan",
  date         = "2015",
  groups       = "Others",
  journaltitle = "arXiv preprint arXiv:1509.02971",
  title        = "Continuous control with deep reinforcement learning",
}

% autonomous driving
@article{RL_Driving_1,
  author       = "Sallab, Ahmad EL and Abdou, Mohammed and Perot, Etienne and Yogamani, Senthil",
  date         = "2017",
  groups       = "Others",
  journaltitle = "Electronic Imaging",
  number       = "19",
  pages        = "70--76",
  publisher    = "Society for Imaging Science and Technology",
  title        = "Deep reinforcement learning framework for autonomous driving",
  volume       = "2017",
}

@article{RL_Driving_2,
  author       = "Kiran, B Ravi and Sobh, Ibrahim and Talpaert, Victor and Mannion, Patrick and Al Sallab, Ahmad A and Yogamani, Senthil and P{\'e}rez, Patrick",
  date         = "2021",
  groups       = "Others",
  journaltitle = "IEEE Transactions on Intelligent Transportation Systems",
  publisher    = "IEEE",
  title        = "Deep reinforcement learning for autonomous driving: A survey",
}

@inproceedings{RL_Driving_3,
  author       = "Osi{\'n}ski, B{\l}a{\.z}ej and Jakubowski, Adam and Zi{\k{e}}cina, Pawe{\l} and Mi{\l}o{\'s}, Piotr and Galias, Christopher and Homoceanu, Silviu and Michalewski, Henryk",
  booktitle    = "2020 IEEE International Conference on Robotics and Automation (ICRA)",
  date         = "2020",
  groups       = "Others",
  organization = "IEEE",
  pages        = "6411--6418",
  title        = "Simulation-based reinforcement learning for real-world autonomous driving",
}

@article{RL_Driving_4,
  author       = "Zhu, Meixin and Wang, Yinhai and Pu, Ziyuan and Hu, Jingyun and Wang, Xuesong and Ke, Ruimin",
  date         = "2020",
  groups       = "Others",
  journaltitle = "Transportation Research Part C: Emerging Technologies",
  pages        = "102662",
  publisher    = "Elsevier",
  title        = "Safe, efficient, and comfortable velocity control based on reinforcement learning for autonomous driving",
  volume       = "117",
}


% game playing
@article{RL_Game_1,
  author       = "Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin",
  date         = "2013",
  groups       = "Others",
  journaltitle = "arXiv preprint arXiv:1312.5602",
  title        = "Playing atari with deep reinforcement learning",
}

@inproceedings{RL_Game_2,
  author       = "Jeerige, Anoop and Bein, Doina and Verma, Abhishek",
  booktitle    = "2019 IEEE 9th Annual Computing and Communication Workshop and Conference (CCWC)",
  date         = "2019",
  groups       = "Others",
  organization = "IEEE",
  pages        = "0366--0371",
  title        = "Comparison of deep reinforcement learning approaches for intelligent game playing",
}

@inproceedings{RL_Game_3,
  author    = "Silver, David and Sutton, Richard S and M{\"u}ller, Martin",
  booktitle = "IJCAI",
  date      = "2007",
  groups    = "Others",
  pages     = "1053--1058",
  title     = "Reinforcement Learning of Local Shape in the Game of Go.",
  volume    = "7",
}

@article{RL_Game_4,
  author       = "Ye, Deheng and Chen, Guibin and Zhang, Wen and Chen, Sheng and Yuan, Bo and Liu, Bo and Chen, Jia and Liu, Zhao and Qiu, Fuhao and Yu, Hongsheng and others",
  date         = "2020",
  groups       = "Others",
  journaltitle = "Advances in Neural Information Processing Systems",
  pages        = "621--632",
  title        = "Towards playing full moba games with deep reinforcement learning",
  volume       = "33",
}



%=======================================================
% RL Applications
%=======================================================

% object manipulation
@inproceedings{RL_Object_1,
  author       = "Matas, Jan and James, Stephen and Davison, Andrew J",
  booktitle    = "Conference on Robot Learning",
  date         = "2018",
  groups       = "Others",
  organization = "PMLR",
  pages        = "734--743",
  title        = "Sim-to-real reinforcement learning for deformable object manipulation",
}

@article{RL_Object_2,
  author       = "Mohammed, Marwan Qaid and Chung, Kwek Lee and Chyi, Chua Shing",
  date         = "2020",
  groups       = "Others",
  journaltitle = "IEEE Access",
  pages        = "178450--178481",
  publisher    = "IEEE",
  title        = "Review of deep reinforcement learning-based object grasping: Techniques, open challenges, and recommendations",
  volume       = "8",
}

@inproceedings{RL_Object_3,
  author       = "Li, Richard and Jabri, Allan and Darrell, Trevor and Agrawal, Pulkit",
  booktitle    = "2020 IEEE International Conference on Robotics and Automation (ICRA)",
  date         = "2020",
  groups       = "Others",
  organization = "IEEE",
  pages        = "4051--4058",
  title        = "Towards practical multi-object manipulation using relational reinforcement learning",
}

@inproceedings{RL_Object_4,
  author       = "Han, Haifeng and Paul, Gavin and Matsubara, Takamitsu",
  booktitle    = "2017 13th IEEE Conference on Automation Science and Engineering (CASE)",
  date         = "2017",
  groups       = "Others",
  organization = "IEEE",
  pages        = "750--755",
  title        = "Model-based reinforcement learning approach for deformable linear object manipulation",
}

@inproceedings{RL_TRPO,
  author       = "Schulman, John and Levine, Sergey and Abbeel, Pieter and Jordan, Michael and Moritz, Philipp",
  booktitle    = "International Conference on Machine Learning",
  date         = "2015",
  groups       = "Others",
  organization = "PMLR",
  pages        = "1889--1897",
  title        = "Trust Region Policy Optimization",
}

@inproceedings{ross2011reduction,
  author       = "Ross, St{\'e}phane and Gordon, Geoffrey and Bagnell, Drew",
  booktitle    = "Proceedings of the fourteenth international conference on artificial intelligence and statistics",
  date         = "2011",
  groups       = "Others",
  organization = "JMLR Workshop and Conference Proceedings",
  pages        = "627--635",
  title        = "A reduction of imitation learning and structured prediction to no-regret online learning",
}

@article{schaal1999imitation,
  author       = "Schaal, Stefan",
  date         = "1999",
  groups       = "Others",
  journaltitle = "Trends in cognitive sciences",
  number       = "6",
  pages        = "233--242",
  publisher    = "Elsevier",
  title        = "Is imitation learning the route to humanoid robots?",
  volume       = "3",
}

@inproceedings{Sermanet2018,
  arxivid    = "1704.06888",
  author     = "Sermanet, Pierre and Lynch, Corey and Chebotar, Yevgen and Hsu, Jasmine and Jang, Eric and Schaal, Stefan and Levine, Sergey and Brain, Google",
  booktitle  = "IEEE International Conference on Robotics and Automation (ICRA)",
  date       = "2018-09",
  doi        = "10.1109/ICRA.2018.8462891",
  eprint     = "1704.06888",
  eprinttype = "arXiv",
  groups     = "Others",
  isbn       = "9781538630815",
  issn       = "10504729",
  location   = "Brisbane, QLD, Australia",
  pages      = "1134--1141",
  publisher  = "Institute of Electrical and Electronics Engineers Inc.",
  title      = "{Time-Contrastive Networks: Self-Supervised Learning from Video}",
  url        = "https://sermanet.github.io/imitate/",
}

@article{Task_Acrobot1,
  author       = "Sutton, Richard S",
  date         = "1996",
  groups       = "Others",
  journaltitle = "Advances in neural information processing systems",
  pages        = "1038--1044",
  publisher    = "Citeseer",
  title        = "Generalization in reinforcement learning: Successful examples using sparse coarse coding",
}

@article{Task_Acrobot2,
  author       = "Geramifard, Alborz and Dann, Christoph and Klein, Robert H and Dabney, William and How, Jonathan P",
  date         = "2015",
  groups       = "Others",
  journaltitle = "J. Mach. Learn. Res.",
  number       = "1",
  pages        = "1573--1578",
  title        = "RLPy: a value-function-based reinforcement learning framework for education and research.",
  volume       = "16",
}

% fine-tuning
@inproceedings{TL_FineTuning_1,
  author    = "V{\'\i}ctor Campos and Pablo Sprechmann and Steven Stenberg Hansen and Andre Barreto and Steven Kapturowski and Alex Vitvitskyi and Adria Puigdomenech Badia and Charles Blundell",
  booktitle = "ICML 2021 Workshop on Unsupervised Reinforcement Learning",
  date      = "2021",
  groups    = "Others",
  title     = "Beyond Fine-Tuning: Transferring Behavior in Reinforcement Learning",
  url       = "https://openreview.net/forum?id=4NUhTHom2HZ",
}

@inproceedings{TL_FineTuning_2,
  author       = "Nagabandi, Anusha and Kahn, Gregory and Fearing, Ronald S and Levine, Sergey",
  booktitle    = "2018 IEEE International Conference on Robotics and Automation (ICRA)",
  date         = "2018",
  groups       = "Others",
  organization = "IEEE",
  pages        = "7559--7566",
  title        = "Neural network dynamics for model-based deep reinforcement learning with model-free fine-tuning",
}


@inproceedings{TL_FineTuning_3,
  author    = "Julian, Ryan and Swanson, Benjamin and Sukhatme, Gaurav and Levine, Sergey and Finn, Chelsea and Hausman, Karol",
  booktitle = "Proceedings of the 2020 Conference on Robot Learning",
  date      = "2021",
  editor    = "Kober, Jens and Ramos, Fabio and Tomlin, Claire",
  groups    = "Others",
  pages     = "2120--2136",
  pdf       = "https://proceedings.mlr.press/v155/julian21a/julian21a.pdf",
  publisher = "PMLR",
  series    = "Proceedings of Machine Learning Research",
  title     = "Never Stop Learning: The Effectiveness of Fine-Tuning in Robotic Reinforcement Learning",
  url       = "https://proceedings.mlr.press/v155/julian21a.html",
  volume    = "155",
}

@inproceedings{TL_Forgetting_2,
  author       = "Serra, Joan and Suris, Didac and Miron, Marius and Karatzoglou, Alexandros",
  booktitle    = "International Conference on Machine Learning",
  date         = "2018",
  groups       = "Others",
  organization = "PMLR",
  pages        = "4548--4557",
  title        = "Overcoming catastrophic forgetting with hard attention to the task",
}



% inter-task mapping
@article{TL_InterTask_1,
  author       = "Taylor, Matthew E. and Stone, Peter and Liu, Yaxin",
  date         = "2007-12",
  groups       = "Others",
  issn         = "1532-4435",
  issue_date   = "12/1/2007",
  journaltitle = "J. Mach. Learn. Res.",
  numpages     = "43",
  pages        = "2125--2167",
  publisher    = "JMLR.org",
  title        = "Transfer Learning via Inter-Task Mappings for Temporal Difference Learning",
  volume       = "8",
}

@article{TL_InterTask_2,
  author       = "Gupta, Abhishek and Devin, Coline and Liu, YuXuan and Abbeel, Pieter and Levine, Sergey",
  date         = "2017",
  groups       = "Others",
  journaltitle = "arXiv preprint arXiv:1703.02949",
  title        = "Learning invariant feature spaces to transfer skills with reinforcement learning",
}

@inproceedings{TL_InterTask_3,
  author       = "Ammar, Haitham B and Tuyls, Karl and Taylor, Matthew E and Driessens, Kurt and Weiss, Gerhard",
  booktitle    = "Proceedings of the 11th international conference on autonomous agents and multiagent systems",
  date         = "2012",
  groups       = "Others",
  organization = "International Foundation for Autonomous Agents and Multiagent Systems~…",
  pages        = "383--390",
  title        = "Reinforcement learning transfer via sparse coding",
  volume       = "1",
}

@article{TL_InterTask_4,
  author       = "Fern{\'a}ndez, Fernando and Garc{\'\i}a, Javier and Veloso, Manuela",
  date         = "2010",
  groups       = "Others",
  journaltitle = "Robotics and Autonomous Systems",
  number       = "7",
  pages        = "866--871",
  publisher    = "Elsevier",
  title        = "Probabilistic policy reuse for inter-task transfer learning",
  volume       = "58",
}

% representation learning
@inproceedings{TL_Representation_1,
  author       = "Devin, Coline and Gupta, Abhishek and Darrell, Trevor and Abbeel, Pieter and Levine, Sergey",
  booktitle    = "2017 IEEE international conference on robotics and automation (ICRA)",
  date         = "2017",
  groups       = "Others",
  organization = "IEEE",
  pages        = "2169--2176",
  title        = "Learning modular neural network policies for multi-task and multi-robot transfer",
}

@inproceedings{TL_Representation_2,
  author    = "Taylor, Matthew E and Stone, Peter",
  booktitle = "AAAI Fall Symposium: Computational Approaches to Representation Change during Learning and Development",
  date      = "2007",
  groups    = "Others",
  pages     = "78--85",
  title     = "Representation Transfer for Reinforcement Learning.",
}

@article{TL_Representation_3,
  author       = "Zhang, Amy and Satija, Harsh and Pineau, Joelle",
  date         = "2018",
  groups       = "Others",
  journaltitle = "arXiv preprint arXiv:1804.10689",
  title        = "Decoupling dynamics and reward for transfer learning",
}

% reward shaping
@article{TL_RewardShaping_1,
  author       = "Mannion, Patrick and Devlin, Sam and Duggan, Jim and Howley, Enda",
  date         = "2018",
  doi          = "10.1017/S0269888918000292",
  groups       = "Others",
  journaltitle = "The Knowledge Engineering Review",
  pages        = "e23",
  publisher    = "Cambridge University Press",
  title        = "Reward shaping for knowledge-based multi-objective multi-agent reinforcement learning",
  volume       = "33",
}

@inproceedings{TL_RewardShaping_2,
  address   = "Richland, SC",
  author    = "Brys, Tim and Harutyunyan, Anna and Taylor, Matthew E. and Now\'{e}, Ann",
  booktitle = "Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems",
  date      = "2015",
  groups    = "Others",
  isbn      = "9781450334136",
  keywords  = "transfer learning, reinforcement learning, reward shaping",
  location  = "Istanbul, Turkey",
  numpages  = "8",
  pages     = "181--188",
  publisher = "International Foundation for Autonomous Agents and Multiagent Systems",
  series    = "AAMAS '15",
  title     = "Policy Transfer Using Reward Shaping",
}

@inproceedings{TL_RewardShaping_3,
  author    = "Doncieux, Stephane",
  booktitle = "2013 IEEE Third Joint International Conference on Development and Learning and Epigenetic Robotics (ICDL)",
  date      = "2013",
  doi       = "10.1109/DevLrn.2013.6652568",
  groups    = "Others",
  pages     = "1--6",
  title     = "Transfer learning for direct policy search: A reward shaping approach",
}

% TL in RL
@article{TL_RL_Survey_1,
  author       = "Taylor, Matthew E. and Stone, Peter",
  date         = "2009-12",
  groups       = "Others",
  issn         = "1532-4435",
  issue_date   = "12/1/2009",
  journaltitle = "J. Mach. Learn. Res.",
  numpages     = "53",
  pages        = "1633--1685",
  publisher    = "JMLR.org",
  title        = "Transfer Learning for Reinforcement Learning Domains: A Survey",
  volume       = "10",
}

@article{TL_RL_Survey_1,
  author       = "Zhuangdi Zhu and Kaixiang Lin and Jiayu Zhou",
  bibsource    = "dblp computer science bibliography, https://dblp.org",
  biburl       = "https://dblp.org/rec/journals/corr/abs-2009-07888.bib",
  date         = "2020",
  eprint       = "2009.07888",
  eprinttype   = "arXiv",
  groups       = "Others",
  journaltitle = "CoRR",
  timestamp    = "Wed, 23 Sep 2020 15:51:46 +0200",
  title        = "Transfer Learning in Deep Reinforcement Learning: {A} Survey",
  url          = "https://arxiv.org/abs/2009.07888",
  volume       = "abs/2009.07888",
}



% robotics
@article{TL_Robotics_1,
  author       = "Hua, Jiang and Zeng, Liangcai and Li, Gongfa and Ju, Zhaojie",
  date         = "2021",
  groups       = "Others",
  journaltitle = "Sensors",
  number       = "4",
  pages        = "1278",
  publisher    = "Multidisciplinary Digital Publishing Institute",
  title        = "Learning for a robot: Deep reinforcement learning, imitation learning, transfer learning",
  volume       = "21",
}

@inproceedings{TL_Robotics_2,
  author       = "Zhao, Wenshuai and Queralta, Jorge Pe{\~n}a and Westerlund, Tomi",
  booktitle    = "2020 IEEE Symposium Series on Computational Intelligence (SSCI)",
  date         = "2020",
  groups       = "Others",
  organization = "IEEE",
  pages        = "737--744",
  title        = "Sim-to-real transfer in deep reinforcement learning for robotics: a survey",
}

@article{TL_Robotics_3,
  author       = "Liu, Yueyue and Li, Zhijun and Liu, Huaping and Kan, Zhen",
  date         = "2020",
  groups       = "Others",
  journaltitle = "Robotics and Autonomous Systems",
  pages        = "103515",
  publisher    = "Elsevier",
  title        = "Skill transfer learning for autonomous robots and human--robot cooperation: A survey",
  volume       = "128",
}



%=======================================================
% Transfer Learning
%=======================================================

% supervised learning
@article{TL_SL_1,
  author       = "Raghu, Maithra and Zhang, Chiyuan and Kleinberg, Jon and Bengio, Samy",
  date         = "2019",
  groups       = "Others",
  journaltitle = "Advances in neural information processing systems",
  title        = "Transfusion: Understanding transfer learning for medical imaging",
  volume       = "32",
}

@article{TL_SL_2,
  author       = "Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J and others",
  date         = "2020",
  groups       = "Others",
  journaltitle = "J. Mach. Learn. Res.",
  number       = "140",
  pages        = "1--67",
  title        = "Exploring the limits of transfer learning with a unified text-to-text transformer.",
  volume       = "21",
}

@article{TL_SL_3,
  author       = "Pathak, Yadunath and Shukla, Prashant Kumar and Tiwari, Akhilesh and Stalin, Shalini and Singh, Saurabh",
  date         = "2020",
  groups       = "Others",
  journaltitle = "Irbm",
  publisher    = "Elsevier",
  title        = "Deep transfer learning based classification model for COVID-19 disease",
}

@article{TL_SL_4,
  author       = "Aslan, Muhammet Fatih and Unlersen, Muhammed Fahri and Sabanci, Kadir and Durdu, Akif",
  date         = "2021",
  groups       = "Others",
  journaltitle = "Applied Soft Computing",
  pages        = "106912",
  publisher    = "Elsevier",
  title        = "CNN-based transfer learning--BiLSTM network: A novel approach for COVID-19 infection detection",
  volume       = "98",
}

@inproceedings{TL_SL_5,
  author       = "Humayun, Mamoona and Sujatha, R and Almuayqil, Saleh Naif and Jhanjhi, NZ",
  booktitle    = "Healthcare",
  date         = "2022",
  groups       = "Others",
  number       = "6",
  organization = "MDPI",
  pages        = "1058",
  title        = "A Transfer Learning Approach with a Convolutional Neural Network for the Classification of Lung Carcinoma",
  volume       = "10",
}

@article{TL_SL_6,
  author       = "Salza, Pasquale and Schwizer, Christoph and Gu, Jian and Gall, Harald C",
  date         = "2022",
  groups       = "Others",
  journaltitle = "IEEE Transactions on Software Engineering",
  publisher    = "IEEE",
  title        = "On the effectiveness of transfer learning for code search",
}

@article{TL_SL_7,
  author       = "Sharma, Mayuri and Nath, Keshab and Sharma, Rupam Kumar and Kumar, Chandan Jyoti and Chaudhary, Ankit",
  date         = "2022",
  groups       = "Others",
  journaltitle = "Electronics",
  number       = "1",
  pages        = "148",
  publisher    = "MDPI",
  title        = "Ensemble averaging of transfer learning models for identification of nutritional deficiency in rice plant",
  volume       = "11",
}

@article{tomov2021multi,
  author       = "Tomov, Momchil S and Schulz, Eric and Gershman, Samuel J",
  date         = "2021",
  groups       = "Others",
  journaltitle = "Nature Human Behaviour",
  pages        = "1--10",
  publisher    = "Nature Publishing Group",
  title        = "Multi-task reinforcement learning in humans",
}

@article{zelinsky2020predicting,
  author       = "Zelinsky, Gregory J and Chen, Yupei and Ahn, Seoyoung and Adeli, Hossein and Yang, Zhibo and Huang, Lihan and Samaras, Dimitrios and Hoai, Minh",
  date         = "2020",
  groups       = "Others",
  journaltitle = "arXiv preprint arXiv:2001.11921",
  title        = "Predicting Goal-directed Attention Control Using Inverse-Reinforcement Learning",
}

@article{zhang2020cgail,
  author       = "Zhang, Xin and Li, Yanhua and Zhou, Xun and Luo, Jun",
  date         = "2020",
  groups       = "Others",
  journaltitle = "IEEE Transactions on Big Data",
  publisher    = "IEEE",
  title        = "cGAIL: Conditional Generative Adversarial Imitation Learning—An Application in Taxi Drivers’ Strategy Learning",
}

@article{zhou2020modeling,
  author       = "Zhou, Yang and Fu, Rui and Wang, Chang and Zhang, Ruibin",
  date         = "2020",
  groups       = "Others",
  journaltitle = "Sensors",
  number       = "18",
  pages        = "5034",
  publisher    = "Multidisciplinary Digital Publishing Institute",
  title        = "Modeling Car-Following Behaviors and Driving Styles with Generative Adversarial Imitation Learning",
  volume       = "20",
}

@inproceedings{ziebart2008maximum,
  author       = "Ziebart, Brian D and Maas, Andrew L and Bagnell, J Andrew and Dey, Anind K",
  booktitle    = "Aaai",
  date         = "2008",
  groups       = "Others",
  organization = "Chicago, IL, USA",
  pages        = "1433--1438",
  title        = "Maximum entropy inverse reinforcement learning.",
  volume       = "8",
}

@article{RL_Bellman1957,
  author       = "Bellman, Richard",
  date         = "1957",
  groups       = "RL",
  issn         = "0095-9057",
  journaltitle = "Journal of Mathematics and Mechanics",
  number       = "5",
  pages        = "679--684",
  publisher    = "Indiana University Mathematics Department",
  title        = "A {Markovian} {Decision} {Process}",
  url          = "https://www.jstor.org/stable/24900506",
  urldate      = "2023-03-14",
  volume       = "6",
}

@book{RL_Sutton2018,
  author     = "Sutton, Richard S. and Barto, Andrew G.",
  date       = "2018",
  groups     = "RL",
  isbn       = "9780262039246",
  location   = "Cambridge, MA, USA",
  publisher  = "A Bradford Book",
  shorttitle = "Reinforcement {Learning}",
  title      = "Reinforcement {Learning}: {An} {Introduction}",
}


@inproceedings{RL_Algo_A3C,
  abstract  = "We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.",
  author    = "Mnih, Volodymyr and Badia, Adrià Puigdomènech and Mirza, Mehdi and Graves, Alex and Harley, Tim and Lillicrap, Timothy P. and Silver, David and Kavukcuoglu, Koray",
  booktitle = "Proceedings of the 33rd {International} {Conference} on {International} {Conference} on {Machine} {Learning} - {Volume} 48",
  date      = "2016-06",
  groups    = "RL_Algo",
  location  = "New York, NY, USA",
  pages     = "1928--1937",
  publisher = "JMLR.org",
  series    = "{ICML}'16",
  title     = "Asynchronous methods for deep reinforcement learning",
  urldate   = "2023-02-16",
}


@techreport{RL_Algo_AlphaZero,
  abstract    = "The game of chess is the most widely-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. In contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go, by tabula rasa reinforcement learning from games of self-play. In this paper, we generalise this approach into a single AlphaZero algorithm that can achieve, tabula rasa, superhuman performance in many challenging domains. Starting from random play, and given no domain knowledge except the game rules, AlphaZero achieved within 24 hours a superhuman level of play in the games of chess and shogi (Japanese chess) as well as Go, and convincingly defeated a world-champion program in each case.",
  author      = "Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and Lillicrap, Timothy and Simonyan, Karen and Hassabis, Demis",
  date        = "2017-12",
  doi         = "10.48550/arXiv.1712.01815",
  file        = ":RL_Algo_Silver2017 - Mastering Chess and Shogi by Self Play with a General Reinforcement Learning Algorithm.pdf:PDF",
  groups      = "RL_Algo",
  institution = "arXiv",
  keywords    = "Computer Science - Artificial Intelligence, Computer Science - Machine Learning",
  note        = "arXiv:1712.01815 [cs] type: article",
  title       = "Mastering {Chess} and {Shogi} by {Self}-{Play} with a {General} {Reinforcement} {Learning} {Algorithm}",
  url         = "http://arxiv.org/abs/1712.01815",
  urldate     = "2023-02-16",
}


@inproceedings{RL_Algo_C51,
  abstract  = "In this paper we argue for the fundamental importance of the value distribution: the distribution of the random return received by a reinforcement learning agent. This is in contrast to the common approach to reinforcement learning which models the expectation of this return, or value. Although there is an established body of literature studying the value distribution, thus far it has always been used for a specific purpose such as implementing risk-aware behaviour. We begin with theoretical results in both the policy evaluation and control settings, exposing a significant distributional instability in the latter. We then use the distributional perspective to design a new algorithm which applies Bellman's equation to the learning of approximate value distributions. We evaluate our algorithm using the suite of games from the Arcade Learning Environment. We obtain both state-of-the-art results and anecdotal evidence demonstrating the importance of the value distribution in approximate reinforcement learning. Finally, we combine theoretical and empirical evidence to highlight the ways in which the value distribution impacts learning in the approximate setting.",
  author    = "Bellemare, Marc G. and Dabney, Will and Munos, Rémi",
  booktitle = "Proceedings of the 34th {International} {Conference} on {Machine} {Learning} - {Volume} 70",
  date      = "2017-08",
  file      = ":RL_Algo_Bellemare2017 - A Distributional Perspective on Reinforcement Learning.pdf:PDF",
  groups    = "RL_Algo",
  location  = "Sydney, NSW, Australia",
  pages     = "449--458",
  publisher = "JMLR.org",
  series    = "{ICML}'17",
  title     = "A {Distributional} {Perspective} on {Reinforcement} {Learning}",
  urldate   = "2023-02-16",
}


@techreport{RL_Algo_DDPG,
  abstract    = "We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.",
  annotation  = "Comment: 10 pages + supplementary",
  author      = "Lillicrap, Timothy P. and Hunt, Jonathan J. and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan",
  date        = "2019-07",
  doi         = "10.48550/arXiv.1509.02971",
  file        = ":RL_Algo_Lillicrap2019 - Continuous Control with Deep Reinforcement Learning.pdf:PDF",
  groups      = "RL_Algo",
  institution = "arXiv",
  keywords    = "Computer Science - Machine Learning, Statistics - Machine Learning",
  note        = "arXiv:1509.02971 [cs, stat] type: article",
  title       = "Continuous control with deep reinforcement learning",
  url         = "http://arxiv.org/abs/1509.02971",
  urldate     = "2023-02-16",
}


@techreport{RL_Algo_DQN,
  abstract    = "We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.",
  annotation  = "Comment: NIPS Deep Learning Workshop 2013",
  author      = "Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin",
  date        = "2013-12",
  doi         = "10.48550/arXiv.1312.5602",
  file        = ":RL_Algo_Mnih2013 - Playing Atari with Deep Reinforcement Learning.pdf:PDF",
  groups      = "RL_Algo",
  institution = "arXiv",
  keywords    = "Computer Science - Machine Learning",
  note        = "arXiv:1312.5602 [cs] type: article",
  title       = "Playing {Atari} with {Deep} {Reinforcement} {Learning}",
  url         = "http://arxiv.org/abs/1312.5602",
  urldate     = "2023-02-16",
}


@inproceedings{RL_Algo_HER,
  abstract  = "Dealing with sparse rewards is one of the biggest challenges in Reinforcement Learning (RL). We present a novel technique called Hindsight Experience Replay which allows sample-efficient learning from rewards which are sparse and binary and therefore avoid the need for complicated reward engineering. It can be combined with an arbitrary off-policy RL algorithm and may be seen as a form of implicit curriculum. We demonstrate our approach on the task of manipulating objects with a robotic arm. In particular, we run experiments on three different tasks: pushing, sliding, and pick-and-place, in each case using only binary rewards indicating whether or not the task is completed. Our ablation studies show that Hindsight Experience Replay is a crucial ingredient which makes training possible in these challenging environments. We show that our policies trained on a physics simulation can be deployed on a physical robot and successfully complete the task. The video presenting our experiments is available at https://goo.gl/SMrQnI.",
  author    = "Andrychowicz, Marcin and Wolski, Filip and Ray, Alex and Schneider, Jonas and Fong, Rachel and Welinder, Peter and McGrew, Bob and Tobin, Josh and Pieter Abbeel, OpenAI and Zaremba, Wojciech",
  booktitle = "Advances in {Neural} {Information} {Processing} {Systems}",
  date      = "2017",
  file      = ":RL_Algo_Andrychowicz2017 - Hindsight Experience Replay.pdf:PDF",
  groups    = "RL_Algo",
  publisher = "Curran Associates, Inc.",
  title     = "Hindsight {Experience} {Replay}",
  url       = "https://papers.nips.cc/paper/2017/hash/453fadbd8a1a3af50a9df4df899537b5-Abstract.html",
  urldate   = "2023-02-16",
  volume    = "30",
}


@inproceedings{RL_Algo_I2A,
  abstract  = "We introduce Imagination-Augmented Agents (I2As), a novel architecture for deep reinforcement learning combining model-free and model-based aspects. In contrast to most existing model-based reinforcement learning and planning methods, which prescribe how a model should be used to arrive at a policy, I2As learn to interpret predictions from a learned environment model to construct implicit plans in arbitrary ways, by using the predictions as additional context in deep policy networks. I2As show improved data efficiency, performance, and robustness to model misspecification compared to several baselines.",
  author    = "Racanière, Sébastien and Weber, Théophane and Reichert, David P. and Buesing, Lars and Guez, Arthur and Rezende, Danilo and Badia, Adria Puigdomènech and Vinyals, Oriol and Heess, Nicolas and Li, Yujia and Pascanu, Razvan and Battaglia, Peter and Hassabis, Demis and Silver, David and Wierstra, Daan",
  booktitle = "Proceedings of the 31st {International} {Conference} on {Neural} {Information} {Processing} {Systems}",
  date      = "2017-12",
  file      = ":RL_Algo_Racaniere2017 - Imagination Augmented Agents for Deep Reinforcement Learning.pdf:PDF",
  groups    = "RL_Algo",
  isbn      = "9781510860964",
  location  = "Red Hook, NY, USA",
  pages     = "5694--5705",
  publisher = "Curran Associates Inc.",
  series    = "{NIPS}'17",
  title     = "Imagination-augmented agents for deep reinforcement learning",
  urldate   = "2023-02-16",
}


@techreport{RL_Algo_MBMF,
  abstract    = "Model-free deep reinforcement learning algorithms have been shown to be capable of learning a wide range of robotic skills, but typically require a very large number of samples to achieve good performance. Model-based algorithms, in principle, can provide for much more efficient learning, but have proven difficult to extend to expressive, high-capacity models such as deep neural networks. In this work, we demonstrate that medium-sized neural network models can in fact be combined with model predictive control (MPC) to achieve excellent sample complexity in a model-based reinforcement learning algorithm, producing stable and plausible gaits to accomplish various complex locomotion tasks. We also propose using deep neural network dynamics models to initialize a model-free learner, in order to combine the sample efficiency of model-based approaches with the high task-specific performance of model-free methods. We empirically demonstrate on MuJoCo locomotion tasks that our pure model-based approach trained on just random action data can follow arbitrary trajectories with excellent sample efficiency, and that our hybrid algorithm can accelerate model-free learning on high-speed benchmark tasks, achieving sample efficiency gains of 3-5x on swimmer, cheetah, hopper, and ant agents. Videos can be found at https://sites.google.com/view/mbmf",
  author      = "Nagabandi, Anusha and Kahn, Gregory and Fearing, Ronald S. and Levine, Sergey",
  date        = "2017-12",
  doi         = "10.48550/arXiv.1708.02596",
  file        = "arXiv Fulltext PDF:https\://arxiv.org/pdf/1708.02596.pdf:application/pdf",
  groups      = "RL_Algo",
  institution = "arXiv",
  keywords    = "Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Robotics",
  note        = "arXiv:1708.02596 [cs] type: article",
  title       = "Neural {Network} {Dynamics} for {Model}-{Based} {Deep} {Reinforcement} {Learning} with {Model}-{Free} {Fine}-{Tuning}",
  url         = "http://arxiv.org/abs/1708.02596",
  urldate     = "2023-02-16",
}


@techreport{RL_Algo_MBVE,
  abstract    = "Recent model-free reinforcement learning algorithms have proposed incorporating learned dynamics models as a source of additional data with the intention of reducing sample complexity. Such methods hold the promise of incorporating imagined data coupled with a notion of model uncertainty to accelerate the learning of continuous control tasks. Unfortunately, they rely on heuristics that limit usage of the dynamics model. We present model-based value expansion, which controls for uncertainty in the model by only allowing imagination to fixed depth. By enabling wider use of learned dynamics models within a model-free reinforcement learning algorithm, we improve value estimation, which, in turn, reduces the sample complexity of learning.",
  author      = "Feinberg, Vladimir and Wan, Alvin and Stoica, Ion and Jordan, Michael I. and Gonzalez, Joseph E. and Levine, Sergey",
  date        = "2018-02",
  doi         = "10.48550/arXiv.1803.00101",
  file        = ":RL_Algo_Feinberg2018 - Model Based Value Estimation for Efficient Model Free Reinforcement Learning.pdf:PDF",
  groups      = "RL_Algo",
  institution = "arXiv",
  keywords    = "Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Statistics - Machine Learning",
  note        = "arXiv:1803.00101 [cs, stat] type: article",
  title       = "Model-{Based} {Value} {Estimation} for {Efficient} {Model}-{Free} {Reinforcement} {Learning}",
  url         = "http://arxiv.org/abs/1803.00101",
  urldate     = "2023-02-16",
}


@techreport{RL_Algo_PPO,
  author      = "Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg",
  date        = "2017-08",
  doi         = "10.48550/arXiv.1707.06347",
  file        = ":RL_Algo_Schulman2017 - Proximal Policy Optimization Algorithms.pdf:PDF",
  groups      = "RL_Algo",
  institution = "arXiv",
  keywords    = "Computer Science - Machine Learning",
  note        = "arXiv:1707.06347 [cs] type: article",
  title       = "Proximal {Policy} {Optimization} {Algorithms}",
  url         = "http://arxiv.org/abs/1707.06347",
  urldate     = "2023-02-16",
}


@inproceedings{RL_Algo_QRDQN,
  abstract  = "In reinforcement learning (RL), an agent interacts with the environment by taking actions and observing the next state and reward. When sampled probabilistically, these state transitions, rewards, and actions can all induce randomness in the observed long-term return. Traditionally, reinforcement learning algorithms average over this randomness to estimate the value function. In this paper, we build on recent work advocating a distributional approach to reinforcement learning in which the distribution over returns is modeled explicitly instead of only estimating the mean. That is, we examine methods of learning the value distribution instead of the value function. We give results that close a number of gaps between the theoretical and algorithmic results given by Bellemare, Dabney, and Munos (2017). First, we extend existing results to the approximate distribution setting. Second, we present a novel distributional reinforcement learning algorithm consistent with our theoretical formulation. Finally, we evaluate this new algorithm on the Atari 2600 games, observing that it significantly outperforms many of the recent improvements on DQN, including the related distributional algorithm C51.",
  author    = "Dabney, Will and Rowland, Mark and Bellemare, Marc G. and Munos, Rémi",
  booktitle = "Proceedings of the {Thirty}-{Second} {AAAI} {Conference} on {Artificial} {Intelligence} and {Thirtieth} {Innovative} {Applications} of {Artificial} {Intelligence} {Conference} and {Eighth} {AAAI} {Symposium} on {Educational} {Advances} in {Artificial} {Intelligence}",
  date      = "2018-02",
  file      = ":RL_Algo_Dabney2018 - Distributional Reinforcement Learning with Quantile Regression.pdf:PDF",
  groups    = "RL_Algo",
  isbn      = "9781577358008",
  location  = "New Orleans, Louisiana, USA",
  pages     = "2892--2901",
  publisher = "AAAI Press",
  series    = "{AAAI}'18/{IAAI}'18/{EAAI}'18",
  title     = "Distributional reinforcement learning with quantile regression",
  urldate   = "2023-02-16",
}

%=======================================================
% Memory effect
%=======================================================

@article{Memory_Repetition_1,
  author  = "Zhan, Lexia and Guo, Dingrong and Chen, Gang and Yang, Jiongjiong",
  doi     = "10.3389/fnhum.2018.00277",
  issn    = "1662-5161",
  journal = "Frontiers in Human Neuroscience",
  title   = "Effects of Repetition Learning on Associative Recognition Over Time: Role of the Hippocampus and Prefrontal Cortex",
  url     = "https://www.frontiersin.org/article/10.3389/fnhum.2018.00277",
  volume  = "12",
  year    = "2018",
}

@article{Memory_Repetition_2,
  author    = "Uchihara, Takumi and Webb, Stuart and Yanagisawa, Akifumi",
  journal   = "Language Learning",
  number    = "3",
  pages     = "559--599",
  publisher = "Wiley Online Library",
  title     = "The effects of repetition on incidental vocabulary learning: A meta-analysis of correlational studies",
  volume    = "69",
  year      = "2019",
}

@article{Memory_Effect_1,
  author    = "Ebbinghaus, Hermann",
  journal   = "Annals of neurosciences",
  number    = "4",
  pages     = "155",
  publisher = "SAGE Publications",
  title     = "Memory: A contribution to experimental psychology",
  volume    = "20",
  year      = "2013",
}



%=======================================================
% RL General
%=======================================================

@book{RL_AnIntroductionBook,
  author    = "Sutton, Richard S and Barto, Andrew G",
  publisher = "MIT press",
  title     = "Reinforcement learning: An introduction",
  year      = "2018",
}

@article{RL_Challenges_1,
  author    = "Dulac-Arnold, Gabriel and Levine, Nir and Mankowitz, Daniel J and Li, Jerry and Paduraru, Cosmin and Gowal, Sven and Hester, Todd",
  journal   = "Machine Learning",
  number    = "9",
  pages     = "2419--2468",
  publisher = "Springer",
  title     = "Challenges of real-world reinforcement learning: definitions, benchmarks and analysis",
  volume    = "110",
  year      = "2021",
}

@article{RL_Challenges_2,
  author    = "Kormushev, Petar and Calinon, Sylvain and Caldwell, Darwin G",
  journal   = "Robotics",
  number    = "3",
  pages     = "122--148",
  publisher = "Multidisciplinary Digital Publishing Institute",
  title     = "Reinforcement learning in robotics: Applications and real-world challenges",
  volume    = "2",
  year      = "2013",
}



%=======================================================
% RL Applications
%=======================================================

% object manipulation
@inproceedings{RL_Object_1,
  author       = "Matas, Jan and James, Stephen and Davison, Andrew J",
  booktitle    = "Conference on Robot Learning",
  organization = "PMLR",
  pages        = "734--743",
  title        = "Sim-to-real reinforcement learning for deformable object manipulation",
  year         = "2018",
}

@article{RL_Object_2,
  author    = "Mohammed, Marwan Qaid and Chung, Kwek Lee and Chyi, Chua Shing",
  journal   = "IEEE Access",
  pages     = "178450--178481",
  publisher = "IEEE",
  title     = "Review of deep reinforcement learning-based object grasping: Techniques, open challenges, and recommendations",
  volume    = "8",
  year      = "2020",
}

@inproceedings{RL_Object_3,
  author       = "Li, Richard and Jabri, Allan and Darrell, Trevor and Agrawal, Pulkit",
  booktitle    = "2020 IEEE International Conference on Robotics and Automation (ICRA)",
  organization = "IEEE",
  pages        = "4051--4058",
  title        = "Towards practical multi-object manipulation using relational reinforcement learning",
  year         = "2020",
}

@inproceedings{RL_Object_4,
  author       = "Han, Haifeng and Paul, Gavin and Matsubara, Takamitsu",
  booktitle    = "2017 13th IEEE Conference on Automation Science and Engineering (CASE)",
  organization = "IEEE",
  pages        = "750--755",
  title        = "Model-based reinforcement learning approach for deformable linear object manipulation",
  year         = "2017",
}


% game playing
@article{RL_Game_1,
  author  = "Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin",
  journal = "arXiv preprint arXiv:1312.5602",
  title   = "Playing atari with deep reinforcement learning",
  year    = "2013",
}

@inproceedings{RL_Game_2,
  author       = "Jeerige, Anoop and Bein, Doina and Verma, Abhishek",
  booktitle    = "2019 IEEE 9th Annual Computing and Communication Workshop and Conference (CCWC)",
  organization = "IEEE",
  pages        = "0366--0371",
  title        = "Comparison of deep reinforcement learning approaches for intelligent game playing",
  year         = "2019",
}

@inproceedings{RL_Game_3,
  author    = "Silver, David and Sutton, Richard S and M{\"u}ller, Martin",
  booktitle = "IJCAI",
  pages     = "1053--1058",
  title     = "Reinforcement Learning of Local Shape in the Game of Go.",
  volume    = "7",
  year      = "2007",
}

@article{RL_Game_4,
  author  = "Ye, Deheng and Chen, Guibin and Zhang, Wen and Chen, Sheng and Yuan, Bo and Liu, Bo and Chen, Jia and Liu, Zhao and Qiu, Fuhao and Yu, Hongsheng and others",
  journal = "Advances in Neural Information Processing Systems",
  pages   = "621--632",
  title   = "Towards playing full moba games with deep reinforcement learning",
  volume  = "33",
  year    = "2020",
}

% autonomous driving
@article{RL_Driving_1,
  author    = "Sallab, Ahmad EL and Abdou, Mohammed and Perot, Etienne and Yogamani, Senthil",
  journal   = "Electronic Imaging",
  number    = "19",
  pages     = "70--76",
  publisher = "Society for Imaging Science and Technology",
  title     = "Deep reinforcement learning framework for autonomous driving",
  volume    = "2017",
  year      = "2017",
}

@article{RL_Driving_2,
  author    = "Kiran, B Ravi and Sobh, Ibrahim and Talpaert, Victor and Mannion, Patrick and Al Sallab, Ahmad A and Yogamani, Senthil and P{\'e}rez, Patrick",
  journal   = "IEEE Transactions on Intelligent Transportation Systems",
  publisher = "IEEE",
  title     = "Deep reinforcement learning for autonomous driving: A survey",
  year      = "2021",
}

@inproceedings{RL_Driving_3,
  author       = "Osi{\'n}ski, B{\l}a{\.z}ej and Jakubowski, Adam and Zi{\k{e}}cina, Pawe{\l} and Mi{\l}o{\'s}, Piotr and Galias, Christopher and Homoceanu, Silviu and Michalewski, Henryk",
  booktitle    = "2020 IEEE International Conference on Robotics and Automation (ICRA)",
  organization = "IEEE",
  pages        = "6411--6418",
  title        = "Simulation-based reinforcement learning for real-world autonomous driving",
  year         = "2020",
}

@article{RL_Driving_4,
  author    = "Zhu, Meixin and Wang, Yinhai and Pu, Ziyuan and Hu, Jingyun and Wang, Xuesong and Ke, Ruimin",
  journal   = "Transportation Research Part C: Emerging Technologies",
  pages     = "102662",
  publisher = "Elsevier",
  title     = "Safe, efficient, and comfortable velocity control based on reinforcement learning for autonomous driving",
  volume    = "117",
  year      = "2020",
}



%=======================================================
% IL General
%=======================================================

@article{IL_Survey_1,
  author   = "Brenna D. Argall and Sonia Chernova and Manuela Veloso and Brett Browning",
  doi      = "https://doi.org/10.1016/j.robot.2008.10.024",
  issn     = "0921-8890",
  journal  = "Robotics and Autonomous Systems",
  keywords = "Learning from demonstration, Robotics, Machine learning, Autonomous systems",
  number   = "5",
  pages    = "469-483",
  title    = "A survey of robot learning from demonstration",
  url      = "https://www.sciencedirect.com/science/article/pii/S0921889008001772",
  volume   = "57",
  year     = "2009",
}

@article{IL_Survey_2,
  author    = "Hussein, Ahmed and Gaber, Mohamed Medhat and Elyan, Eyad and Jayne, Chrisina",
  journal   = "ACM Computing Surveys (CSUR)",
  number    = "2",
  pages     = "1--35",
  publisher = "ACM New York, NY, USA",
  title     = "Imitation learning: A survey of learning methods",
  volume    = "50",
  year      = "2017",
}



%=======================================================
% IL Applications
%=======================================================

% robotics control
@inproceedings{IL_Robotics_1,
  author       = "Jang, Eric and Irpan, Alex and Khansari, Mohi and Kappler, Daniel and Ebert, Frederik and Lynch, Corey and Levine, Sergey and Finn, Chelsea",
  booktitle    = "Conference on Robot Learning",
  organization = "PMLR",
  pages        = "991--1002",
  title        = "BC-z: Zero-shot task generalization with robotic imitation learning",
  year         = "2022",
}

@article{IL_Robotics_2,
  author  = "Zhu, Yuke and Wang, Ziyu and Merel, Josh and Rusu, Andrei and Erez, Tom and Cabi, Serkan and Tunyasuvunakool, Saran and Kram{\'a}r, J{\'a}nos and Hadsell, Raia and de Freitas, Nando and others",
  journal = "arXiv preprint arXiv:1802.09564",
  title   = "Reinforcement and imitation learning for diverse visuomotor skills",
  year    = "2018",
}

@inproceedings{IL_Robotics_3,
  author       = "Ratliff, Nathan and Bagnell, J Andrew and Srinivasa, Siddhartha S",
  booktitle    = "2007 7th IEEE-RAS International Conference on Humanoid Robots",
  organization = "IEEE",
  pages        = "392--397",
  title        = "Imitation learning for locomotion and manipulation",
  year         = "2007",
}


% autonomous driving
@inproceedings{IL_Driving_1,
  author       = "Chen, Jianyu and Yuan, Bodi and Tomizuka, Masayoshi",
  booktitle    = "2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)",
  organization = "IEEE",
  pages        = "2884--2890",
  title        = "Deep imitation learning for autonomous driving in generic urban scenarios with enhanced safety",
  year         = "2019",
}

@inproceedings{IL_Driving_2,
  author       = "Codevilla, Felipe and M{\"u}ller, Matthias and L{\'o}pez, Antonio and Koltun, Vladlen and Dosovitskiy, Alexey",
  booktitle    = "2018 IEEE international conference on robotics and automation (ICRA)",
  organization = "IEEE",
  pages        = "4693--4700",
  title        = "End-to-end driving via conditional imitation learning",
  year         = "2018",
}

@inproceedings{IL_Driving_3,
  author       = "Hawke, Jeffrey and Shen, Richard and Gurau, Corina and Sharma, Siddharth and Reda, Daniele and Nikolov, Nikolay and Mazur, Przemys{\l}aw and Micklethwaite, Sean and Griffiths, Nicolas and Shah, Amar and others",
  booktitle    = "2020 IEEE International Conference on Robotics and Automation (ICRA)",
  organization = "IEEE",
  pages        = "251--257",
  title        = "Urban driving with conditional imitation learning",
  year         = "2020",
}

@inproceedings{IL_Driving_4,
  author       = "Kebria, Parham M and Alizadehsani, Roohallah and Salaken, Syed Moshfeq and Hossain, Ibrahim and Khosravi, Abbas and Kabir, Dipu and Koohestani, Afsaneh and Asadi, Houshyar and Nahavandi, Saeid and Tunsel, Edward and others",
  booktitle    = "2019 IEEE International Conference on Industrial Technology (ICIT)",
  organization = "IEEE",
  pages        = "865--870",
  title        = "Evaluating architecture impacts on deep imitation learning performance for autonomous driving",
  year         = "2019",
}



%=======================================================
% Transfer Learning
%=======================================================

% supervised learning
@article{TL_SL_1,
  author  = "Raghu, Maithra and Zhang, Chiyuan and Kleinberg, Jon and Bengio, Samy",
  journal = "Advances in neural information processing systems",
  title   = "Transfusion: Understanding transfer learning for medical imaging",
  volume  = "32",
  year    = "2019",
}

@article{TL_SL_2,
  author  = "Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J and others",
  journal = "J. Mach. Learn. Res.",
  number  = "140",
  pages   = "1--67",
  title   = "Exploring the limits of transfer learning with a unified text-to-text transformer.",
  volume  = "21",
  year    = "2020",
}

@article{TL_SL_3,
  author    = "Pathak, Yadunath and Shukla, Prashant Kumar and Tiwari, Akhilesh and Stalin, Shalini and Singh, Saurabh",
  journal   = "Irbm",
  publisher = "Elsevier",
  title     = "Deep transfer learning based classification model for COVID-19 disease",
  year      = "2020",
}

@article{TL_SL_4,
  author    = "Aslan, Muhammet Fatih and Unlersen, Muhammed Fahri and Sabanci, Kadir and Durdu, Akif",
  journal   = "Applied Soft Computing",
  pages     = "106912",
  publisher = "Elsevier",
  title     = "CNN-based transfer learning--BiLSTM network: A novel approach for COVID-19 infection detection",
  volume    = "98",
  year      = "2021",
}

@inproceedings{TL_SL_5,
  author       = "Humayun, Mamoona and Sujatha, R and Almuayqil, Saleh Naif and Jhanjhi, NZ",
  booktitle    = "Healthcare",
  number       = "6",
  organization = "MDPI",
  pages        = "1058",
  title        = "A Transfer Learning Approach with a Convolutional Neural Network for the Classification of Lung Carcinoma",
  volume       = "10",
  year         = "2022",
}

@article{TL_SL_6,
  author    = "Salza, Pasquale and Schwizer, Christoph and Gu, Jian and Gall, Harald C",
  journal   = "IEEE Transactions on Software Engineering",
  publisher = "IEEE",
  title     = "On the effectiveness of transfer learning for code search",
  year      = "2022",
}

@article{TL_SL_7,
  author    = "Sharma, Mayuri and Nath, Keshab and Sharma, Rupam Kumar and Kumar, Chandan Jyoti and Chaudhary, Ankit",
  journal   = "Electronics",
  number    = "1",
  pages     = "148",
  publisher = "MDPI",
  title     = "Ensemble averaging of transfer learning models for identification of nutritional deficiency in rice plant",
  volume    = "11",
  year      = "2022",
}



% robotics
@article{TL_Robotics_1,
  author    = "Hua, Jiang and Zeng, Liangcai and Li, Gongfa and Ju, Zhaojie",
  journal   = "Sensors",
  number    = "4",
  pages     = "1278",
  publisher = "Multidisciplinary Digital Publishing Institute",
  title     = "Learning for a robot: Deep reinforcement learning, imitation learning, transfer learning",
  volume    = "21",
  year      = "2021",
}

@inproceedings{TL_Robotics_2,
  author       = "Zhao, Wenshuai and Queralta, Jorge Pe{\~n}a and Westerlund, Tomi",
  booktitle    = "2020 IEEE Symposium Series on Computational Intelligence (SSCI)",
  organization = "IEEE",
  pages        = "737--744",
  title        = "Sim-to-real transfer in deep reinforcement learning for robotics: a survey",
  year         = "2020",
}

@article{TL_Robotics_3,
  author    = "Liu, Yueyue and Li, Zhijun and Liu, Huaping and Kan, Zhen",
  journal   = "Robotics and Autonomous Systems",
  pages     = "103515",
  publisher = "Elsevier",
  title     = "Skill transfer learning for autonomous robots and human--robot cooperation: A survey",
  volume    = "128",
  year      = "2020",
}

% castastrophic forgetting
@article{TL_Forgetting_1,
  author    = "Vithayathil Varghese, Nelson and Mahmoud, Qusay H",
  journal   = "Electronics",
  number    = "9",
  pages     = "1363",
  publisher = "Multidisciplinary Digital Publishing Institute",
  title     = "A survey of multi-task deep reinforcement learning",
  volume    = "9",
  year      = "2020",
}

@inproceedings{TL_Forgetting_2,
  author       = "Serra, Joan and Suris, Didac and Miron, Marius and Karatzoglou, Alexandros",
  booktitle    = "International Conference on Machine Learning",
  organization = "PMLR",
  pages        = "4548--4557",
  title        = "Overcoming catastrophic forgetting with hard attention to the task",
  year         = "2018",
}

% fine-tuning
@inproceedings{TL_FineTuning_1,
  author    = "V{\'\i}ctor Campos and Pablo Sprechmann and Steven Stenberg Hansen and Andre Barreto and Steven Kapturowski and Alex Vitvitskyi and Adria Puigdomenech Badia and Charles Blundell",
  booktitle = "ICML 2021 Workshop on Unsupervised Reinforcement Learning",
  title     = "Beyond Fine-Tuning: Transferring Behavior in Reinforcement Learning",
  url       = "https://openreview.net/forum?id=4NUhTHom2HZ",
  year      = "2021",
}

@inproceedings{TL_FineTuning_2,
  author       = "Nagabandi, Anusha and Kahn, Gregory and Fearing, Ronald S and Levine, Sergey",
  booktitle    = "2018 IEEE International Conference on Robotics and Automation (ICRA)",
  organization = "IEEE",
  pages        = "7559--7566",
  title        = "Neural network dynamics for model-based deep reinforcement learning with model-free fine-tuning",
  year         = "2018",
}


@inproceedings{TL_FineTuning_3,
  author    = "Julian, Ryan and Swanson, Benjamin and Sukhatme, Gaurav and Levine, Sergey and Finn, Chelsea and Hausman, Karol",
  booktitle = "Proceedings of the 2020 Conference on Robot Learning",
  editor    = "Kober, Jens and Ramos, Fabio and Tomlin, Claire",
  month     = "16--18 Nov",
  pages     = "2120--2136",
  pdf       = "https://proceedings.mlr.press/v155/julian21a/julian21a.pdf",
  publisher = "PMLR",
  series    = "Proceedings of Machine Learning Research",
  title     = "Never Stop Learning: The Effectiveness of Fine-Tuning in Robotic Reinforcement Learning",
  url       = "https://proceedings.mlr.press/v155/julian21a.html",
  volume    = "155",
  year      = "2021",
}

% reward shaping
@article{TL_RewardShaping_1,
  author    = "Mannion, Patrick and Devlin, Sam and Duggan, Jim and Howley, Enda",
  doi       = "10.1017/S0269888918000292",
  journal   = "The Knowledge Engineering Review",
  pages     = "e23",
  publisher = "Cambridge University Press",
  title     = "Reward shaping for knowledge-based multi-objective multi-agent reinforcement learning",
  volume    = "33",
  year      = "2018",
}

@inproceedings{TL_RewardShaping_2,
  address   = "Richland, SC",
  author    = "Brys, Tim and Harutyunyan, Anna and Taylor, Matthew E. and Now\'{e}, Ann",
  booktitle = "Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems",
  isbn      = "9781450334136",
  keywords  = "transfer learning, reinforcement learning, reward shaping",
  location  = "Istanbul, Turkey",
  numpages  = "8",
  pages     = "181–188",
  publisher = "International Foundation for Autonomous Agents and Multiagent Systems",
  series    = "AAMAS '15",
  title     = "Policy Transfer Using Reward Shaping",
  year      = "2015",
}

@inproceedings{TL_RewardShaping_3,
  author    = "Doncieux, Stephane",
  booktitle = "2013 IEEE Third Joint International Conference on Development and Learning and Epigenetic Robotics (ICDL)",
  doi       = "10.1109/DevLrn.2013.6652568",
  number    = "",
  pages     = "1-6",
  title     = "Transfer learning for direct policy search: A reward shaping approach",
  volume    = "",
  year      = "2013",
}



% inter-task mapping
@article{TL_InterTask_1,
  author     = "Taylor, Matthew E. and Stone, Peter and Liu, Yaxin",
  issn       = "1532-4435",
  issue_date = "12/1/2007",
  journal    = "J. Mach. Learn. Res.",
  month      = "dec",
  numpages   = "43",
  pages      = "2125–2167",
  publisher  = "JMLR.org",
  title      = "Transfer Learning via Inter-Task Mappings for Temporal Difference Learning",
  volume     = "8",
  year       = "2007",
}

@article{TL_InterTask_2,
  author  = "Gupta, Abhishek and Devin, Coline and Liu, YuXuan and Abbeel, Pieter and Levine, Sergey",
  journal = "arXiv preprint arXiv:1703.02949",
  title   = "Learning invariant feature spaces to transfer skills with reinforcement learning",
  year    = "2017",
}

@inproceedings{TL_InterTask_3,
  author       = "Ammar, Haitham B and Tuyls, Karl and Taylor, Matthew E and Driessens, Kurt and Weiss, Gerhard",
  booktitle    = "Proceedings of the 11th international conference on autonomous agents and multiagent systems",
  organization = "International Foundation for Autonomous Agents and Multiagent Systems~…",
  pages        = "383--390",
  title        = "Reinforcement learning transfer via sparse coding",
  volume       = "1",
  year         = "2012",
}

@article{TL_InterTask_4,
  author    = "Fern{\'a}ndez, Fernando and Garc{\'\i}a, Javier and Veloso, Manuela",
  journal   = "Robotics and Autonomous Systems",
  number    = "7",
  pages     = "866--871",
  publisher = "Elsevier",
  title     = "Probabilistic policy reuse for inter-task transfer learning",
  volume    = "58",
  year      = "2010",
}

% representation learning
@inproceedings{TL_Representation_1,
  author       = "Devin, Coline and Gupta, Abhishek and Darrell, Trevor and Abbeel, Pieter and Levine, Sergey",
  booktitle    = "2017 IEEE international conference on robotics and automation (ICRA)",
  organization = "IEEE",
  pages        = "2169--2176",
  title        = "Learning modular neural network policies for multi-task and multi-robot transfer",
  year         = "2017",
}

@inproceedings{TL_Representation_2,
  author    = "Taylor, Matthew E and Stone, Peter",
  booktitle = "AAAI Fall Symposium: Computational Approaches to Representation Change during Learning and Development",
  pages     = "78--85",
  title     = "Representation Transfer for Reinforcement Learning.",
  year      = "2007",
}

@article{TL_Representation_3,
  author  = "Zhang, Amy and Satija, Harsh and Pineau, Joelle",
  journal = "arXiv preprint arXiv:1804.10689",
  title   = "Decoupling dynamics and reward for transfer learning",
  year    = "2018",
}

% TL in RL
@article{TL_RL_Survey_1,
  author     = "Taylor, Matthew E. and Stone, Peter",
  issn       = "1532-4435",
  issue_date = "12/1/2009",
  journal    = "J. Mach. Learn. Res.",
  month      = "dec",
  numpages   = "53",
  pages      = "1633–1685",
  publisher  = "JMLR.org",
  title      = "Transfer Learning for Reinforcement Learning Domains: A Survey",
  volume     = "10",
  year       = "2009",
}

@article{TL_RL_Survey_1,
  author     = "Zhuangdi Zhu and Kaixiang Lin and Jiayu Zhou",
  bibsource  = "dblp computer science bibliography, https://dblp.org",
  biburl     = "https://dblp.org/rec/journals/corr/abs-2009-07888.bib",
  eprint     = "2009.07888",
  eprinttype = "arXiv",
  journal    = "CoRR",
  timestamp  = "Wed, 23 Sep 2020 15:51:46 +0200",
  title      = "Transfer Learning in Deep Reinforcement Learning: {A} Survey",
  url        = "https://arxiv.org/abs/2009.07888",
  volume     = "abs/2009.07888",
  year       = "2020",
}

%
@inproceedings{Baseline_TATL,
  autho\w      = "Joshi, Girish and Chowdhary, Girish",
  booktitle    = "2018 IEEE International Conference on Robotics and Automation (ICRA)",
  organization = "IEEE",
  pages        = "7525--7532",
  title        = "Cross-domain transfer in reinforcement learning using target apprentice",
  year         = "2018",
}



%=======================================================
% Multi-task Learning
%=======================================================

@article{MTL_Survey,
  article-number = "1363",
  author         = "Vithayathil Varghese, Nelson and Mahmoud, Qusay H.",
  doi            = "10.3390/electronics9091363",
  issn           = "2079-9292",
  journal        = "Electronics",
  number         = "9",
  title          = "A Survey of Multi-Task Deep Reinforcement Learning",
  url            = "https://www.mdpi.com/2079-9292/9/9/1363",
  volume         = "9",
  year           = "2020",
}

@inproceedings{MTL_1,
  author       = "Guo, Zhaohan Daniel and Pires, Bernardo Avila and Piot, Bilal and Grill, Jean-Bastien and Altch{\'e}, Florent and Munos, R{\'e}mi and Azar, Mohammad Gheshlaghi",
  booktitle    = "International Conference on Machine Learning",
  organization = "PMLR",
  pages        = "3875--3886",
  title        = "Bootstrap latent-predictive representations for multitask reinforcement learning",
  year         = "2020",
}

@inproceedings{MTL_2,
  author       = "Rahmatizadeh, Rouhollah and Abolghasemi, Pooya and B{\"o}l{\"o}ni, Ladislau and Levine, Sergey",
  booktitle    = "2018 IEEE international conference on robotics and automation (ICRA)",
  organization = "IEEE",
  pages        = "3758--3765",
  title        = "Vision-based multi-task manipulation for inexpensive robots using end-to-end learning from demonstration",
  year         = "2018",
}

@article{MTL_3,
  author  = "Teh, Yee and Bapst, Victor and Czarnecki, Wojciech M and Quan, John and Kirkpatrick, James and Hadsell, Raia and Heess, Nicolas and Pascanu, Razvan",
  journal = "Advances in neural information processing systems",
  title   = "Distral: Robust multitask reinforcement learning",
  volume  = "30",
  year    = "2017",
}

@inproceedings{MTL_4,
  author       = "Espeholt, Lasse and Soyer, Hubert and Munos, Remi and Simonyan, Karen and Mnih, Vlad and Ward, Tom and Doron, Yotam and Firoiu, Vlad and Harley, Tim and Dunning, Iain and others",
  booktitle    = "International Conference on Machine Learning",
  organization = "PMLR",
  pages        = "1407--1416",
  title        = "Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures",
  year         = "2018",
}

@inproceedings{MTL_5,
  author    = "Hessel, Matteo and Soyer, Hubert and Espeholt, Lasse and Czarnecki, Wojciech and Schmitt, Simon and van Hasselt, Hado",
  booktitle = "Proceedings of the AAAI Conference on Artificial Intelligence",
  number    = "01",
  pages     = "3796--3803",
  title     = "Multi-task deep reinforcement learning with popart",
  volume    = "33",
  year      = "2019",
}



%=======================================================
% RL Models
%=======================================================

@article{Baseline_PPO,
  author  = "Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg",
  journal = "arXiv preprint arXiv:1707.06347",
  title   = "Proximal policy optimization algorithms",
  year    = "2017",
}

@inproceedings{Baseline_NFQI,
  author       = "Riedmiller, Martin",
  booktitle    = "European conference on machine learning",
  organization = "Springer",
  pages        = "317--328",
  title        = "Neural fitted Q iteration--first experiences with a data efficient neural reinforcement learning method",
  year         = "2005",
}





%=======================================================
% IL Models
%=======================================================

@inproceedings{IL_Model_GAIL,
  author    = "Ho, Jonathan and Ermon, Stefano",
  booktitle = "Advances in Neural Information Processing Systems",
  publisher = "Curran Associates, Inc.",
  title     = "{Generative Adversarial Imitation Learning}",
  url       = "https://proceedings.neurips.cc/paper/2016/hash/cc7e2b878868cbae992d1fb743995d8f-Abstract.html",
  volume    = "29",
  year      = "2016",
}



%=======================================================
% GAN
%=======================================================

@article{GAN_Original,
  archiveprefix = "arXiv",
  arxivid       = "1406.2661",
  author        = "Goodfellow, Ian J and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua",
  eprint        = "1406.2661",
  journal       = "arXiv",
  month         = "jun",
  title         = "{Generative Adversarial Networks}",
  url           = "http://arxiv.org/abs/1406.2661",
  year          = "2014",
}



%=======================================================
% Contrastive learning
%=======================================================

@inproceedings{CL_Stopgrad_1,
  author       = "Tian, Yuandong and Chen, Xinlei and Ganguli, Surya",
  booktitle    = "International Conference on Machine Learning",
  organization = "PMLR",
  pages        = "10268--10278",
  title        = "Understanding self-supervised learning dynamics without contrastive pairs",
  year         = "2021",
}

@inproceedings{CL_Stopgrad_2,
  author    = "Chen, Xinlei and He, Kaiming",
  booktitle = "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
  pages     = "15750--15758",
  title     = "Exploring simple siamese representation learning",
  year      = "2021",
}

@article{GAN_CAPTCHA,
  author  = "Hyun KWON and Yongchul KIM and Hyunsoo YOON and Daeseon CHOI",
  doi     = "10.1587/transinf.2017EDL8175",
  journal = "IEICE Transactions on Information and Systems",
  number  = "2",
  pages   = "543-546",
  title   = "CAPTCHA Image Generation Systems Using Generative Adversarial Networks",
  volume  = "E101.D",
  year    = "2018",
}
@article{GAN_MRI,
  author    = "Qi, Mengke and Li, Yongbao and Wu, Aiqian and Jia, Qiyuan and Li, Bin and Sun, Wenzhao and Dai, Zhenhui and Lu, Xingyu and Zhou, Linghong and Deng, Xiaowu and others",
  journal   = "Medical physics",
  number    = "4",
  pages     = "1880--1894",
  publisher = "Wiley Online Library",
  title     = "Multi-sequence MR image-based synthetic CT generation using a generative adversarial network for head and neck MRI-only radiotherapy",
  volume    = "47",
  year      = "2020",
}
@article{GAN_ImageCompression,
  author    = "Song, Jingkuan and He, Tao and Gao, Lianli and Xu, Xing and Hanjalic, Alan and Shen, Heng Tao",
  journal   = "International Journal of Computer Vision",
  pages     = "1--22",
  publisher = "Springer",
  title     = "Unified binary generative adversarial network for image retrieval and compression",
  year      = "2020",
}
@inproceedings{GAN_ImageTranslation,
  author    = "Chen, Hanting and Wang, Yunhe and Shu, Han and Wen, Changyuan and Xu, Chunjing and Shi, Boxin and Xu, Chao and Xu, Chang",
  booktitle = "Proceedings of the AAAI Conference on Artificial Intelligence",
  number    = "04",
  pages     = "3585--3592",
  title     = "Distilling portable generative adversarial networks for image translation",
  volume    = "34",
  year      = "2020",
}


@book{RL_AnIntroductionBook,
  author    = "Sutton, Richard S and Barto, Andrew G",
  publisher = "MIT press",
  title     = "Reinforcement learning: An introduction",
  year      = "2018",
}

@inproceedings{Mujoco_HAPTIX,
  author       = "Kumar, Vikash and Todorov, Emanuel",
  booktitle    = "2015 IEEE-RAS 15th International Conference on Humanoid Robots (Humanoids)",
  organization = "IEEE",
  pages        = "657--663",
  title        = "Mujoco haptix: A virtual reality system for hand manipulation",
  year         = "2015",
}


%=======================================================
% IL General
%=======================================================

@inproceedings{baker2007goal,
  author    = "Baker, Chris L and Tenenbaum, Joshua B and Saxe, Rebecca R",
  booktitle = "Proceedings of the Annual Meeting of the Cognitive Science Society",
  number    = "29",
  title     = "Goal inference as inverse planning",
  volume    = "29",
  year      = "2007",
}

@inproceedings{ziebart2008maximum,
  author       = "Ziebart, Brian D and Maas, Andrew L and Bagnell, J Andrew and Dey, Anind K",
  booktitle    = "Aaai",
  organization = "Chicago, IL, USA",
  pages        = "1433--1438",
  title        = "Maximum entropy inverse reinforcement learning.",
  volume       = "8",
  year         = "2008",
}

@inproceedings{ng2000algorithms,
  author    = "Ng, Andrew Y and Russell, Stuart J and others",
  booktitle = "ICML",
  pages     = "2",
  title     = "Algorithms for inverse reinforcement learning.",
  volume    = "1",
  year      = "2000",
}

@inproceedings{abbeel2004apprenticeship,
  author    = "Abbeel, Pieter and Ng, Andrew Y",
  booktitle = "Proceedings of the twenty-first international conference on Machine learning",
  pages     = "1",
  title     = "Apprenticeship learning via inverse reinforcement learning",
  year      = "2004",
}

@article{duan2017one,
  author  = "Duan, Yan and Andrychowicz, Marcin and Stadie, Bradly C and Ho, Jonathan and Schneider, Jonas and Sutskever, Ilya and Abbeel, Pieter and Zaremba, Wojciech",
  journal = "arXiv preprint arXiv:1703.07326",
  title   = "One-shot imitation learning",
  year    = "2017",
}

@inproceedings{Sermanet2018,
  address       = "Brisbane, QLD, Australia",
  archiveprefix = "arXiv",
  arxivid       = "1704.06888",
  author        = "Sermanet, Pierre and Lynch, Corey and Chebotar, Yevgen and Hsu, Jasmine and Jang, Eric and Schaal, Stefan and Levine, Sergey and Brain, Google",
  booktitle     = "IEEE International Conference on Robotics and Automation (ICRA)",
  doi           = "10.1109/ICRA.2018.8462891",
  eprint        = "1704.06888",
  isbn          = "9781538630815",
  issn          = "10504729",
  month         = "sep",
  pages         = "1134--1141",
  publisher     = "Institute of Electrical and Electronics Engineers Inc.",
  title         = "{Time-Contrastive Networks: Self-Supervised Learning from Video}",
  url           = "https://sermanet.github.io/imitate/",
  year          = "2018",
}

@article{arora2021survey,
  author    = "Arora, Saurabh and Doshi, Prashant",
  journal   = "Artificial Intelligence",
  pages     = "103500",
  publisher = "Elsevier",
  title     = "A survey of inverse reinforcement learning: Challenges, methods and progress",
  year      = "2021",
}


@inproceedings{kalakrishnan2013learning,
  author       = "Kalakrishnan, Mrinal and Pastor, Peter and Righetti, Ludovic and Schaal, Stefan",
  booktitle    = "2013 IEEE International Conference on Robotics and Automation",
  organization = "IEEE",
  pages        = "1331--1336",
  title        = "Learning objective functions for manipulation",
  year         = "2013",
}
@inproceedings{finn2016guided,
  author       = "Finn, Chelsea and Levine, Sergey and Abbeel, Pieter",
  booktitle    = "International conference on machine learning",
  organization = "PMLR",
  pages        = "49--58",
  title        = "Guided cost learning: Deep inverse optimal control via policy optimization",
  year         = "2016",
}


@inproceedings{Liu2018,
  archiveprefix   = "arXiv",
  arxivid         = "1707.03374",
  author          = "Liu, Yuxuan and Gupta, Abhishek and Abbeel, Pieter and Levine, Sergey",
  booktitle       = "IEEE International Conference on Robotics and Automation (ICRA)",
  doi             = "10.1109/ICRA.2018.8462901",
  eprint          = "1707.03374",
  isbn            = "9781538630815",
  issn            = "10504729",
  mendeley-groups = "_Imitation Learning/_UNREAD",
  month           = "sep",
  pages           = "1118--1125",
  publisher       = "Institute of Electrical and Electronics Engineers Inc.",
  title           = "{Imitation from Observation: Learning to Imitate Behaviors from Raw Video via Context Translation}",
  year            = "2018",
}


@article{Liu2019,
  archiveprefix = "arXiv",
  arxivid       = "1911.10947",
  author        = "Liu, Fangchen and Ling, Zhan and Mu, Tongzhou and Su, Hao",
  eprint        = "1911.10947",
  journal       = "arXiv",
  month         = "nov",
  publisher     = "arXiv",
  title         = "{State Alignment-based Imitation Learning}",
  url           = "http://arxiv.org/abs/1911.10947",
  year          = "2019",
}

@inproceedings{ross2011reduction,
  author       = "Ross, St{\'e}phane and Gordon, Geoffrey and Bagnell, Drew",
  booktitle    = "Proceedings of the fourteenth international conference on artificial intelligence and statistics",
  organization = "JMLR Workshop and Conference Proceedings",
  pages        = "627--635",
  title        = "A reduction of imitation learning and structured prediction to no-regret online learning",
  year         = "2011",
}

@article{schaal1999imitation,
  author    = "Schaal, Stefan",
  journal   = "Trends in cognitive sciences",
  number    = "6",
  pages     = "233--242",
  publisher = "Elsevier",
  title     = "Is imitation learning the route to humanoid robots?",
  volume    = "3",
  year      = "1999",
}

@article{bao2017imitation,
  author    = "Bao, Yunqing and Cuijpers, Raymond H",
  journal   = "International Journal of Social Robotics",
  number    = "5",
  pages     = "691--703",
  publisher = "Springer",
  title     = "On the imitation of goal directed movements of a humanoid robot",
  volume    = "9",
  year      = "2017",
}

@article{tomov2021multi,
  author    = "Tomov, Momchil S and Schulz, Eric and Gershman, Samuel J",
  journal   = "Nature Human Behaviour",
  pages     = "1--10",
  publisher = "Nature Publishing Group",
  title     = "Multi-task reinforcement learning in humans",
  year      = "2021",
}


@article{bing2020energy,
  author    = "Bing, Zhenshan and Lemke, Christian and Cheng, Long and Huang, Kai and Knoll, Alois",
  journal   = "Neural Networks",
  pages     = "323--333",
  publisher = "Elsevier",
  title     = "Energy-efficient and damage-recovery slithering gait design for a snake-like robot based on reinforcement learning and inverse reinforcement learning",
  volume    = "129",
  year      = "2020",
}
@article{zelinsky2020predicting,
  author  = "Zelinsky, Gregory J and Chen, Yupei and Ahn, Seoyoung and Adeli, Hossein and Yang, Zhibo and Huang, Lihan and Samaras, Dimitrios and Hoai, Minh",
  journal = "arXiv preprint arXiv:2001.11921",
  title   = "Predicting Goal-directed Attention Control Using Inverse-Reinforcement Learning",
  year    = "2020",
}
@inproceedings{naumann2020analyzing,
  author       = "Naumann, Maximilian and Sun, Liting and Zhan, Wei and Tomizuka, Masayoshi",
  booktitle    = "2020 IEEE International Conference on Robotics and Automation (ICRA)",
  organization = "IEEE",
  pages        = "5481--5487",
  title        = "Analyzing the Suitability of Cost Functions for Explaining and Imitating Human Driving Behavior based on Inverse Reinforcement Learning",
  year         = "2020",
}




@inproceedings{chi2020collaborative,
  author       = "Chi, Wenqiang and Dagnino, Giulio and Kwok, Trevor MY and Nguyen, Anh and Kundrat, Dennis and Abdelaziz, Mohamed EMK and Riga, Celia and Bicknell, Colin and Yang, Guang-Zhong",
  booktitle    = "2020 IEEE International Conference on Robotics and Automation (ICRA)",
  organization = "IEEE",
  pages        = "2414--2420",
  title        = "Collaborative robot-assisted endovascular catheterization with generative adversarial imitation learning",
  year         = "2020",
}
@article{zhou2020modeling,
  author    = "Zhou, Yang and Fu, Rui and Wang, Chang and Zhang, Ruibin",
  journal   = "Sensors",
  number    = "18",
  pages     = "5034",
  publisher = "Multidisciplinary Digital Publishing Institute",
  title     = "Modeling Car-Following Behaviors and Driving Styles with Generative Adversarial Imitation Learning",
  volume    = "20",
  year      = "2020",
}
@article{zhang2020cgail,
  author    = "Zhang, Xin and Li, Yanhua and Zhou, Xun and Luo, Jun",
  journal   = "IEEE Transactions on Big Data",
  publisher = "IEEE",
  title     = "cGAIL: Conditional Generative Adversarial Imitation Learning—An Application in Taxi Drivers’ Strategy Learning",
  year      = "2020",
}



@techreport{pomerleau1989alvinn,
  author      = "Pomerleau, Dean A",
  institution = "CARNEGIE-MELLON UNIV PITTSBURGH PA ARTIFICIAL INTELLIGENCE AND PSYCHOLOGY~…",
  title       = "Alvinn: An autonomous land vehicle in a neural network",
  year        = "1989",
}

@inproceedings{rahmatizadeh2018vision,
  author       = "Rahmatizadeh, Rouhollah and Abolghasemi, Pooya and B{\"o}l{\"o}ni, Ladislau and Levine, Sergey",
  booktitle    = "2018 IEEE international conference on robotics and automation (ICRA)",
  organization = "IEEE",
  pages        = "3758--3765",
  title        = "Vision-based multi-task manipulation for inexpensive robots using end-to-end learning from demonstration",
  year         = "2018",
}




@inproceedings{10.5555/3294996.3295138,
  abstract  = "The goal of imitation learning is to mimic expert behavior without access to an explicit reward signal. Expert demonstrations provided by humans, however, often show significant variability due to latent factors that are typically not explicitly modeled. In this paper, we propose a new algorithm that can infer the latent structure of expert demonstrations in an unsupervised way. Our method, built on top of Generative Adversarial Imitation Learning, can not only imitate complex behaviors, but also learn interpretable and meaningful representations of complex behavioral data, including visual demonstrations. In the driving domain, we show that a model learned from human demonstrations is able to both accurately reproduce a variety of behaviors and accurately anticipate human actions using raw visual inputs. Compared with various baselines, our method can better capture the latent structure underlying expert demonstrations, often recovering semantically meaningful factors of variation in the data.",
  address   = "Red Hook, NY, USA",
  author    = "Li, Yunzhu and Song, Jiaming and Ermon, Stefano",
  booktitle = "Proceedings of the 31st International Conference on Neural Information Processing Systems",
  isbn      = "9781510860964",
  location  = "Long Beach, California, USA",
  numpages  = "11",
  pages     = "3815–3825",
  publisher = "Curran Associates Inc.",
  series    = "NIPS'17",
  title     = "InfoGAIL: Interpretable Imitation Learning from Visual Demonstrations",
  year      = "2017",
}


@inproceedings{behbahani2019learning,
  author       = "Behbahani, Feryal and Shiarlis, Kyriacos and Chen, Xi and Kurin, Vitaly and Kasewa, Sudhanshu and Stirbu, Ciprian and Gomes, Joao and Paul, Supratik and Oliehoek, Frans A and Messias, Joao and others",
  booktitle    = "2019 International Conference on Robotics and Automation (ICRA)",
  organization = "IEEE",
  pages        = "775--781",
  title        = "Learning from demonstration in the wild",
  year         = "2019",
}
 @inproceedings{pmlr-v70-baram17a,
  abstract  = "Generative Adversarial Networks (GANs) have been successfully applied to the problem of policy imitation in a model-free setup. However, the computation graph of GANs, that include a stochastic policy as the generative model, is no longer differentiable end-to-end, which requires the use of high-variance gradient estimation. In this paper, we introduce the Model-based Generative Adversarial Imitation Learning (MGAIL) algorithm. We show how to use a forward model to make the computation fully differentiable, which enables training policies using the exact gradient of the discriminator. The resulting algorithm trains competent policies using relatively fewer expert samples and interactions with the environment. We test it on both discrete and continuous action domains and report results that surpass the state-of-the-art.",
  author    = "Nir Baram and Oron Anschel and Itai Caspi and Shie Mannor",
  booktitle = "Proceedings of the 34th International Conference on Machine Learning",
  editor    = "Precup, Doina and Teh, Yee Whye",
  month     = "06--11 Aug",
  pages     = "390--399",
  pdf       = "http://proceedings.mlr.press/v70/baram17a/baram17a.pdf",
  publisher = "PMLR",
  series    = "Proceedings of Machine Learning Research",
  title     = "End-to-End Differentiable Adversarial Imitation Learning",
  url       = " http://proceedings.mlr.press/v70/baram17a.html ",
  volume    = "70",
  year      = "2017",
}


%=======================================================
% RL Models
%=======================================================

@article{RL_DDPG,
  author  = "Lillicrap, Timothy P and Hunt, Jonathan J and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan",
  journal = "arXiv preprint arXiv:1509.02971",
  title   = "Continuous control with deep reinforcement learning",
  year    = "2015",
}

@inproceedings{RL_TRPO,
  author       = "Schulman, John and Levine, Sergey and Abbeel, Pieter and Jordan, Michael and Moritz, Philipp",
  booktitle    = "International Conference on Machine Learning",
  organization = "PMLR",
  pages        = "1889--1897",
  title        = "Trust Region Policy Optimization",
  year         = "2015",
}

@article{levine2011nonlinear,
  author  = "Levine, Sergey and Popovic, Zoran and Koltun, Vladlen",
  journal = "Advances in neural information processing systems",
  pages   = "19--27",
  title   = "Nonlinear inverse reinforcement learning with gaussian processes",
  volume  = "24",
  year    = "2011",
}



%=======================================================
% IL General
%=======================================================

@article{IL_Survey_RobotLearning,
  author   = "Brenna D. Argall and Sonia Chernova and Manuela Veloso and Brett Browning",
  doi      = "https://doi.org/10.1016/j.robot.2008.10.024",
  issn     = "0921-8890",
  journal  = "Robotics and Autonomous Systems",
  keywords = "Learning from demonstration, Robotics, Machine learning, Autonomous systems",
  number   = "5",
  pages    = "469-483",
  title    = "A survey of robot learning from demonstration",
  url      = "https://www.sciencedirect.com/science/article/pii/S0921889008001772",
  volume   = "57",
  year     = "2009",
}



%=======================================================
% IL Models
%=======================================================

@inproceedings{IL_Model_GAIL,
  author    = "Ho, Jonathan and Ermon, Stefano",
  booktitle = "Advances in Neural Information Processing Systems",
  publisher = "Curran Associates, Inc.",
  title     = "{Generative Adversarial Imitation Learning}",
  url       = "https://proceedings.neurips.cc/paper/2016/hash/cc7e2b878868cbae992d1fb743995d8f-Abstract.html",
  volume    = "29",
  year      = "2016",
}

%=======================================================
% IL DA Models
%=======================================================

@inproceedings{DAIL_Model_DAIL,
  author    = "Kim, Kuno and Gu, Yihong and Song, Jiaming and Zhao, Shengjia and Ermon, Stefano",
  booktitle = "Proceedings of the 37th International Conference on Machine Learning",
  issn      = "2640-3498",
  month     = "nov",
  pages     = "5286--5295",
  publisher = "PMLR",
  title     = "{Domain Adaptive Imitation Learning}",
  url       = "http://proceedings.mlr.press/v119/kim20c.html",
  volume    = "119",
  year      = "2020",
}

@inproceedings{DAIL_Model_ThirdPerson,
  address       = "Toulon, France",
  archiveprefix = "arXiv",
  arxivid       = "1703.01703",
  author        = "Stadie, Bradly C. and Abbeel, Pieter and Sutskever, Ilya",
  booktitle     = "International Conference on Learning Representations (ICLR)",
  eprint        = "1703.01703",
  month         = "apr",
  title         = "{Third-Person Imitation Learning}",
  url           = "http://arxiv.org/abs/1703.01703",
  year          = "2017",
}

%=======================================================
% GANs
%=======================================================

@article{GAN_Original,
  archiveprefix = "arXiv",
  arxivid       = "1406.2661",
  author        = "Goodfellow, Ian J and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua",
  eprint        = "1406.2661",
  journal       = "arXiv",
  month         = "jun",
  title         = "{Generative Adversarial Networks}",
  url           = "http://arxiv.org/abs/1406.2661",
  year          = "2014",
}

%=======================================================
% Tasks
%=======================================================
@article{Task_OpenAIGym,
  author  = "Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech",
  journal = "arXiv preprint arXiv:1606.01540",
  title   = "Openai gym",
  year    = "2016",
}

@article{Task_CartPole,
  author    = "Barto, Andrew G and Sutton, Richard S and Anderson, Charles W",
  journal   = "IEEE transactions on systems, man, and cybernetics",
  number    = "5",
  pages     = "834--846",
  publisher = "IEEE",
  title     = "Neuronlike adaptive elements that can solve difficult learning control problems",
  year      = "1983",
}

@inproceedings{Task_Adroit,
  author    = "Rajeswaran, Aravind and Kumar, Vikash and Gupta, Abhishek and Vezzani, Giulia and Schulman, John and Todorov, Emanuel and Levine, Sergey",
  booktitle = "Proceedings of Robotics: Science and Systems (RSS)",
  title     = "{Learning Complex Dexterous Manipulation with Deep Reinforcement Learning and Demonstrations}",
  url       = "https://sites.google.com/view/deeprl-dexterous-manipulation",
  year      = "2018",
}

@article{Task_Acrobot1,
  author    = "Sutton, Richard S",
  journal   = "Advances in neural information processing systems",
  pages     = "1038--1044",
  publisher = "Citeseer",
  title     = "Generalization in reinforcement learning: Successful examples using sparse coarse coding",
  year      = "1996",
}

@article{Task_Acrobot2,
  author  = "Geramifard, Alborz and Dann, Christoph and Klein, Robert H and Dabney, William and How, Jonathan P",
  journal = "J. Mach. Learn. Res.",
  number  = "1",
  pages   = "1573--1578",
  title   = "RLPy: a value-function-based reinforcement learning framework for education and research.",
  volume  = "16",
  year    = "2015",
}






%=======================================================
% Simulated tasks
%=======================================================

@article{Env_OpenAIGym,
  author  = "Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech",
  journal = "arXiv preprint arXiv:1606.01540",
  title   = "OpenAI Gym",
  year    = "2016",
}

@inproceedings{Env_MetaWorld,
  author    = "Yu, Tianhe and Quillen, Deirdre and He, Zhanpeng and Julian, Ryan and Hausman, Karol and Finn, Chelsea and Levine, Sergey",
  booktitle = "Proceedings of the Conference on Robot Learning",
  editor    = "Kaelbling, Leslie Pack and Kragic, Danica and Sugiura, Komei",
  month     = "30 Oct--01 Nov",
  pages     = "1094--1100",
  pdf       = "http://proceedings.mlr.press/v100/yu20a/yu20a.pdf",
  publisher = "PMLR",
  series    = "Proceedings of Machine Learning Research",
  title     = "Meta-World: A Benchmark and Evaluation for Multi-Task and Meta Reinforcement Learning",
  url       = "https://proceedings.mlr.press/v100/yu20a.html",
  volume    = "100",
  year      = "2020",
}

@inproceedings{Env_Adroit,
  address   = "Pittsburgh, Pennsylvania",
  author    = "Rajeswaran, Aravind and Kumar, Vikash and Gupta, Abhishek and Vezzani, Giulia and Schulman, John and Todorov, Emanuel and Levine, Sergey",
  booktitle = "Proceedings of Robotics: Science and Systems (RSS)",
  doi       = "10.15607/RSS.2018.XIV.049",
  month     = "June",
  title     = "{Learning Complex Dexterous Manipulation with Deep Reinforcement Learning and Demonstrations}",
  url       = "https://sites.google.com/view/deeprl-dexterous-manipulation",
  year      = "2018",
}

@article{Env_CartPole,
  author    = "Barto, Andrew G and Sutton, Richard S and Anderson, Charles W",
  journal   = "IEEE transactions on systems, man, and cybernetics",
  number    = "5",
  pages     = "834--846",
  publisher = "IEEE",
  title     = "Neuronlike adaptive elements that can solve difficult learning control problems",
  year      = "1983",
}



%=======================================================
% DL Frameworks
%=======================================================

@article{DL_Lib_PyTorch,
  author  = "Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others",
  journal = "Advances in neural information processing systems",
  title   = "Pytorch: An imperative style, high-performance deep learning library",
  volume  = "32",
  year    = "2019",
}

@article{DL_Lib_Tianshou,
  author  = "Weng, Jiayi and Chen, Huayu and Yan, Dong and You, Kaichao and Duburcq, Alexis and Zhang, Minghao and Su, Hang and Zhu, Jun",
  journal = "arXiv preprint arXiv:2107.14171",
  title   = "Tianshou: A Highly Modularized Deep Reinforcement Learning Library",
  year    = "2021",
}

@article{DL_Lib_StableBaselines3,
  author    = "Raffin, Antonin and Hill, Ashley and Gleave, Adam and Kanervisto, Anssi and Ernestus, Maximilian and Dormann, Noah",
  journal   = "Journal of Machine Learning Research",
  publisher = "MIT Press",
  title     = "Stable-baselines3: Reliable reinforcement learning implementations",
  year      = "2021",
}


@inproceedings{RL_Algo_SAC,
  abstract   = "Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.",
  author     = "Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey",
  date       = "2018-07",
  file       = ":haarnoja_soft_2018 - Soft Actor Critic_ off Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor.pdf:PDF;:RL_Algo_Haarnoja2018 - Soft Actor Critic_ off Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor.pdf:PDF",
  groups     = "RL_Algo",
  issn       = "2640-3498",
  language   = "en",
  pages      = "1861--1870",
  publisher  = "PMLR",
  shorttitle = "Soft {Actor}-{Critic}",
  title      = "Soft {Actor}-{Critic}: {Off}-{Policy} {Maximum} {Entropy} {Deep} {Reinforcement} {Learning} with a {Stochastic} {Actor}",
  url        = "https://proceedings.mlr.press/v80/haarnoja18b.html",
  urldate    = "2023-02-16",
}


@inproceedings{RL_Algo_TD3,
  abstract  = "In value-based reinforcement learning methods such as deep Q-learning, function approximation errors are known to lead to overestimated value estimates and suboptimal policies. We show that this problem persists in an actor-critic setting and propose novel mechanisms to minimize its effects on both the actor and the critic. Our algorithm builds on Double Q-learning, by taking the minimum value between a pair of critics to limit overestimation. We draw the connection between target networks and overestimation bias, and suggest delaying policy updates to reduce per-update error and further improve performance. We evaluate our method on the suite of OpenAI gym tasks, outperforming the state of the art in every environment tested.",
  author    = "Fujimoto, Scott and Hoof, Herke and Meger, David",
  date      = "2018-07",
  file      = ":fujimoto_addressing_2018 - Addressing Function Approximation Error in Actor Critic Methods.pdf:PDF;:RL_Algo_Fujimoto2018 - Addressing Function Approximation Error in Actor Critic Methods.pdf:PDF",
  groups    = "RL_Algo",
  issn      = "2640-3498",
  language  = "en",
  pages     = "1587--1596",
  publisher = "PMLR",
  title     = "Addressing {Function} {Approximation} {Error} in {Actor}-{Critic} {Methods}",
  url       = "https://proceedings.mlr.press/v80/fujimoto18a.html",
  urldate   = "2023-02-16",
}


@inproceedings{RL_Algo_TRPO,
  abstract  = "In this article, we describe a method for optimizing control policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified scheme, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters.",
  author    = "Schulman, John and Levine, Sergey and Moritz, Philipp and Jordan, Michael and Abbeel, Pieter",
  booktitle = "Proceedings of the 32nd {International} {Conference} on {International} {Conference} on {Machine} {Learning} - {Volume} 37",
  date      = "2015-07",
  groups    = "RL_Algo",
  location  = "Lille, France",
  pages     = "1889--1897",
  publisher = "JMLR.org",
  series    = "{ICML}'15",
  title     = "Trust region policy optimization",
  urldate   = "2023-02-16",
}


@inproceedings{RL_Algo_WM,
  abstract  = "A generative recurrent neural network is quickly trained in an unsupervised manner to model popular reinforcement learning environments through compressed spatio-temporal representations. The world model's extracted features are fed into compact and simple policies trained by evolution, achieving state of the art results in various environments. We also train our agent entirely inside of an environment generated by its own internal world model, and transfer this policy back into the actual environment. Interactive version of this paper is available at https://worldmodels.github.io",
  author    = "Ha, David and Schmidhuber, Jürgen",
  booktitle = "Advances in {Neural} {Information} {Processing} {Systems}",
  date      = "2018",
  file      = ":RL_Algo_Ha2018 - Recurrent World Models Facilitate Policy Evolution.pdf:PDF",
  groups    = "RL_Algo",
  publisher = "Curran Associates, Inc.",
  title     = "Recurrent {World} {Models} {Facilitate} {Policy} {Evolution}",
  url       = "https://papers.nips.cc/paper/2018/hash/2de5d16682c3c35007e4e92982f1a2ba-Abstract.html",
  urldate   = "2023-02-16",
  volume    = "31",
}


@article{RobotApplications_Covid19_Javaid2020,
  abstract     = "The COVID-19 outbreak has resulted in the manufacturing and service sectors being badly hit globally. Since there are no vaccines or any proven medical treatment available, there is an urgent need to take necessary steps to prevent the spread of this virus. As the virus spreads with human-to-human interaction, lockdown has been declared in many countries, and the public is advised to observe social distancing strictly. Robots can undertake human-like activities and can be gainfully programmed to replace some of the human interactions. Through this paper, we identify and propose the introduction of robots to take up this challenge in the fight against the COVID-19 pandemic. We did a comprehensive review of the literature to identify robots’ possible applications in the management of epidemics and pandemics of this nature. We have reviewed the available literature through the search engines of PubMed, SCOPUS, Google Scholar, and Research Gate. A comprehensive review of the literature identified different types of robots being used in the medical field. We could find several vital applications of robots in the management of the COVID-19 pandemic. No doubt technology comes with a cost. In this paper, we identified how different types of robots are used gainfully to deliver medicine, food, and other essential items to COVID-19 patients who are under quarantine. Therefore, there is extensive scope for customising robots to undertake hazardous and repetitive jobs with precision and reliability.",
  author       = "Javaid, Mohd and Haleem, Abid and Vaish, Abhishek and Vaishya, Raju and Iyengar, Karthikeyan P",
  date         = "2020-12",
  doi          = "10.1142/S2424862220300033",
  file         = "Full Text PDF:https\://www.worldscientific.com/doi/pdf/10.1142/S2424862220300033:application/pdf",
  groups       = "RobotApplications_Covid19",
  issn         = "2424-8622",
  journaltitle = "Journal of Industrial Integration and Management",
  keywords     = "Applications of robots, COVID-19, medical, robotics, supply, Applications of robots, COVID-19, medical, robotics, supply",
  number       = "04",
  pages        = "441--451",
  publisher    = "World Scientific Publishing Co.",
  shorttitle   = "Robotics {Applications} in {COVID}-19",
  title        = "Robotics {Applications} in {COVID}-19: {A} {Review}",
  url          = "https://www.worldscientific.com/doi/abs/10.1142/S2424862220300033",
  volume       = "05",
}


@article{RobotApplications_Covid19_Jiang2021,
  abstract     = "Various intelligent technologies have been applied during COVID-19, which has become a worldwide public health emergency and brought significant challenges to the medical systems around the world. Notably, the application of robots has played a role in hospitals, quarantine facilities and public spaces and has attracted much attention from the media and the public. This study is based on a questionnaire survey on the perception and reception of robots used for medical care in the pandemic among the Chinese population. A total of 1667 people participated in the survey, 93.6\% of respondents were pursuing or had completed a bachelor, master or even doctorate degree. The results show that Chinese people generally held positive attitudes towards “anti-pandemic robots” and affirmed their contribution to reducing the burden of medical care and virus transmission. A few respondents were concerned about the issues of robots replacing humans and it was apparent that their ethical views on robots were not completely consistent across their demographics (e.g., age, industry). Nevertheless, most respondents tended to be optimistic about robot applications and dialectical about the ethical issues involved. This is related to the prominent role robots played during the pandemic, the Chinese public’s expectations of new technologies and technology-friendly public opinion in China. Exploring the perception and reception of anti-pandemic robots in different countries or cultures is important because it can shed some light on the future applications of robots, especially in the field of infectious disease control.",
  author       = "Jiang, Hui and Cheng, Lin",
  copyright    = "http://creativecommons.org/licenses/by/3.0/",
  date         = "2021-01",
  doi          = "10.3390/ijerph182010908",
  file         = ":RobotApplications_Covid19_Jiang2021 - Public Perception and Reception of Robotic Applications in Public Health Emergencies Based on a Questionnaire Survey Conducted during COVID 19.pdf:PDF",
  groups       = "RobotApplications_Covid19",
  issn         = "1660-4601",
  journaltitle = "International Journal of Environmental Research and Public Health",
  keywords     = "anti-pandemic robots, COVID-19, China, public awareness, replacement of humans by robots, roboethics",
  language     = "en",
  number       = "20",
  pages        = "10908",
  publisher    = "Multidisciplinary Digital Publishing Institute",
  title        = "Public {Perception} and {Reception} of {Robotic} {Applications} in {Public} {Health} {Emergencies} {Based} on a {Questionnaire} {Survey} {Conducted} during {COVID}-19",
  url          = "https://www.mdpi.com/1660-4601/18/20/10908",
  volume       = "18",
}


@article{RobotApplications_Covid19_Raje2021,
  abstract     = "Due to the increasing number of COVID-19 cases, there is a remarkable demand for robots, especially in the clinical sector. SARS-CoV-2 mainly propagates due to close human interactions and contaminated surfaces, and hence, maintaining social distancing has become a mandatory preventive measure. This generates the need to treat patients with minimal doctor-patient interaction. Introducing robots in the healthcare sector protects the frontline healthcare workers from getting exposed to the coronavirus as well as decreases the need for medical personnel as robots can partially take over some medical roles. The aim of this paper is to highlight the emerging role of robotic applications in the healthcare sector and allied areas. To this end, a systematic review was conducted regarding the various robots that have been implemented worldwide during the COVID-19 pandemic to attenuate and contain the virus. The results obtained from this study reveal that the implementation of robotics into the healthcare field has a substantial effect in controlling the spread of SARS-CoV-2, as it blocks coronavirus propagation between patients and healthcare workers, along with other advantages such as disinfection or cleaning.",
  author       = "Raje, Shruti and Reddy, Nikunj and Jerbi, Houssem and Randhawa, Princy and Tsaramirsis, Georgios and Shrivas, Nikhil Vivek and Pavlopoulou, Athanasia and Stojmenović, Miloš and Piromalis, Dimitris",
  date         = "2021",
  doi          = "10.1155/2021/7099510",
  file         = ":RobotApplications_Covid19_Raje2021 - Applications of Healthcare Robots in Combating the COVID 19 Pandemic.html:URL",
  groups       = "RobotApplications_Covid19",
  issn         = "1176-2322",
  journaltitle = "Applied Bionics and Biomechanics",
  language     = "eng",
  pages        = "7099510",
  pmcid        = "PMC8611354",
  pmid         = "34840604",
  title        = "Applications of {Healthcare} {Robots} in {Combating} the {COVID}-19 {Pandemic}",
  volume       = "2021",
}


@article{RobotApplications_Covid19_Shen2021,
  abstract     = "As a result of the difficulties brought by COVID-19 and its associated lockdowns, many individuals and companies have turned to robots in order to overcome the challenges of the pandemic. Compared with traditional human labor, robotic and autonomous systems have advantages such as an intrinsic immunity to the virus and an inability for human-robot-human spread of any disease-causing pathogens, though there are still many technical hurdles for the robotics industry to overcome. This survey comprehensively reviews over 200 reports covering robotic systems which have emerged or have been repurposed during the past several months, to provide insights to both academia and industry. In each chapter, we cover both the advantages and the challenges for each robot, finding that robotics systems are overall apt solutions for dealing with many of the problems brought on by COVID-19, including: diagnosis, screening, disinfection, surgery, telehealth, care, logistics, manufacturing and broader interpersonal problems unique to the lockdowns of the pandemic. By discussing the potential new robot capabilities and fields they applied to, we expect the robotics industry to take a leap forward due to this unexpected pandemic.",
  author       = "Shen, Yang and Guo, Dejun and Long, Fei and Mateos, Luis A. and Ding, Houzhu and Xiu, Zhen and Hellman, Randall B. and King, Adam and Chen, Shixun and Zhang, Chengkun and Tan, Huan",
  date         = "2021",
  doi          = "10.1109/ACCESS.2020.3045792",
  file         = ":RobotApplications_Covid19_Shen2021 - Robots under COVID 19 Pandemic_ a Comprehensive Survey.html:URL",
  groups       = "RobotApplications_Covid19",
  issn         = "2169-3536",
  journaltitle = "IEEE access: practical innovations, open solutions",
  keywords     = "COVID-19, SARS-CoV-2, autonomous systems, drones, learning, public health, review, robots, sensors, survey",
  language     = "eng",
  pages        = "1590--1615",
  pmcid        = "PMC8675561",
  pmid         = "34976569",
  shorttitle   = "Robots {Under} {COVID}-19 {Pandemic}",
  title        = "Robots {Under} {COVID}-19 {Pandemic}: {A} {Comprehensive} {Survey}",
  volume       = "9",
}


@misc{RobotApplications_Covid19_Spectrum,
  abstract = "Autonomous machines proved their worth in hospitals, offices, and on city streets",
  groups   = "RobotApplications_Covid19",
  language = "en",
  title    = "How {Robots} {Became} {Essential} {Workers} in the {COVID}-19 {Response} - {IEEE} {Spectrum}",
  url      = "https://spectrum.ieee.org/how-robots-became-essential-workers-in-the-covid19-response",
  urldate  = "2023-03-01",
}

@Comment{jabref-meta: databaseType:biblatex;}

@Comment{jabref-meta: grouping:
0 AllEntriesGroup:;
1 StaticGroup:DLFramwork\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:IL\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:IL_BC\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:IL_IRL\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:Mine\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:Neuroscience\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:Others\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:RL\;0\;0\;0x8a8a8aff\;\;\;;
1 StaticGroup:RL_Algo\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:RobotApplications_Covid19\;0\;1\;0x8a8a8aff\;\;\;;
}

@Comment{jabref-meta: protectedFlag:true;}

@Comment{jabref-meta: saveActions:enabled;
all-text-fields[identity]
date[normalize_date]
month[normalize_month]
pages[normalize_page_numbers]
;}

@Comment{jabref-meta: saveOrderConfig:specified;groups;false;citationkey;false;}
